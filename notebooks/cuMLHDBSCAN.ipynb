{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1cdeb0f-2c13-4659-bb99-e69e819db86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "# from hdbscan import HDBSCAN\n",
    "from cuml.cluster import HDBSCAN, DBSCAN, KMeans\n",
    "# from sklearn.decomposition import TruncatedSVD \n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score\n",
    "from cuml.metrics.cluster.entropy import cython_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637ae79a-4a0a-432d-a5ce-b0eaeb3d7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(Path.cwd().parent, 'data/interim')\n",
    "labels_dir = Path(data_dir, 'labels')\n",
    "models_dir = Path(Path.cwd().parent, 'models')\n",
    "mod_paths = sorted([mod_path for mod_path in Path(models_dir).glob('d2v*.model')])[2:]\n",
    "# mod_paths = sorted([str(mod_path) for mod_path in Path(models_dir).glob('*.model')])[1:] # removing 1e5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afe7052-b920-49c2-90bb-6bead14c4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(path, data=None):\n",
    "    if not path.is_file():\n",
    "        with open(path, 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(data.keys())\n",
    "            if data is not None:\n",
    "                writer.writerow(data.values())\n",
    "    else:\n",
    "        with open(path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(data.values())\n",
    "            \n",
    "\n",
    "def load_labels(file):\n",
    "    with open(file, 'rb') as handle:\n",
    "        labels = pkl.load(handle)\n",
    "            \n",
    "    return labels\n",
    "\n",
    "\n",
    "def export_labels(labels, file):\n",
    "    with open(file, 'wb') as handle:\n",
    "        pkl.dump(labels, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def get_kmeans(data, dataset, distance, n_clusters):\n",
    "    labels_file = Path(data_dir, f'labels_cuMLkm_{dataset}_{n_clusters:02d}_{distance}.pkl')\n",
    "    if labels_file.is_file():\n",
    "        labels = load_labels(labels_file)\n",
    "    else:\n",
    "        km = KMeans(n_clusters=n_clusters)\n",
    "        km.fit(data)\n",
    "        labels = km.labels_.tolist()\n",
    "        export_labels(labels, labels_file)\n",
    "        \n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_dbscan(data, dataset, distance, epsilon, min_pts):\n",
    "    labels_file = Path(data_dir, f'labels_cuMLdbscan_{dataset}_{epsilon}_{min_pts:02d}_{distance}.pkl')\n",
    "    if labels_file.is_file():\n",
    "        labels = load_labels(labels_file)\n",
    "    else:\n",
    "        db = DBSCAN(eps=epsilon,\n",
    "                    min_samples=min_pts,\n",
    "                    metric=distance\n",
    "                   ).fit(data)\n",
    "        labels = db.labels_\n",
    "        export_labels(labels, labels_file)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_hdbscan(data, dataset, distance, min_clt_size, min_samples):\n",
    "    labels_file = Path(data_dir, f'labels_cuMLhdbscan_{dataset}_{min_clt_size:02d}_{min_samples:02d}_{distance}.pkl')\n",
    "    if labels_file.is_file():\n",
    "        labels = load_labels(labels_file)\n",
    "    else:\n",
    "        clusterer = HDBSCAN(min_cluster_size=min_clt_size,\n",
    "                            min_samples=min_samples,\n",
    "                            metric=distance\n",
    "                           ).fit(data)\n",
    "        labels = clusterer.labels_\n",
    "        export_labels(labels, labels_file)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def evaluate_cluster(data, labels, distance, method, n_clusters=None):\n",
    "    results = {}\n",
    "    \n",
    "    if method != 'km':\n",
    "        count_clust = Counter(labels)\n",
    "        n_clusters = len([key for key in count_clust.keys() if key != -1])\n",
    "        results['n_clusters'] = n_clusters\n",
    "\n",
    "        if -1 in count_clust:\n",
    "            n_noise = count_clust[-1]\n",
    "            results['n_noise'] = n_noise\n",
    "\n",
    "        clust_data = []\n",
    "        clust_labs = []\n",
    "        for i, label in enumerate(labels):\n",
    "            if label != -1:\n",
    "                clust_data.append(data[i])\n",
    "                clust_labs.append(labels[i])\n",
    "    else:\n",
    "        clust_data = data\n",
    "        clust_labs = labels\n",
    "        n_clusters = n_clusters\n",
    "        results['n_clusters'] = n_clusters\n",
    "    \n",
    "    clust_data = np.asarray(clust_data)\n",
    "    clust_labs = np.asarray(clust_labs)\n",
    "    \n",
    "    if len(clust_labs) == n_clusters or n_clusters < 2:\n",
    "        results.update({'sl_score': None, 'ch_score': None, 'db_score': None, 'entropy': None})\n",
    "    else:\n",
    "        if method != 'km':\n",
    "            results.update({\n",
    "                'sl_score': cython_silhouette_score(clust_data, clust_labs, metric=distance),\n",
    "                'ch_score': calinski_harabasz_score(clust_data, clust_labs),\n",
    "                'db_score': davies_bouldin_score(clust_data, clust_labs),\n",
    "                'entropy' : cython_entropy(clust_labs)\n",
    "            })\n",
    "        else:\n",
    "            results.update({\n",
    "                'sl_score': cython_silhouette_score(clust_data, clust_labs, metric=distance),\n",
    "                'ch_score': calinski_harabasz_score(clust_data, clust_labs),\n",
    "                'db_score': davies_bouldin_score(clust_data, clust_labs),\n",
    "                'entropy' : None\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(data, filename, labels, dataset, distance, method, **kwargs):\n",
    "    if method=='km':\n",
    "        n_clusters = kwargs['n_clusters']\n",
    "        cls_res = evaluate_cluster(data, labels, distance, method, n_clusters)\n",
    "    elif method=='dbscan':\n",
    "        cls_res = evaluate_cluster(data, labels, distance, method)\n",
    "        cls_res = {'epsilon': kwargs['epsilon'], 'min_pts': kwargs['min_pts']}  | cls_res\n",
    "    elif method=='hdbscan':\n",
    "        cls_res = evaluate_cluster(data, labels, distance, method)\n",
    "        cls_res = {'min_clt_size': kwargs['min_clt_size'], 'min_samples': kwargs['min_samples']}  | cls_res\n",
    "    \n",
    "    results = {'distance': distance, 'dataset': dataset} | cls_res\n",
    "    export_results(Path(data_dir, filename + '.csv'), results)\n",
    "\n",
    "\n",
    "def clustering(path, method='km', **kwargs):\n",
    "    if 'distance' not in kwargs:\n",
    "        distance = 'euclidean'\n",
    "    else:\n",
    "        distance = kwargs['distance']\n",
    "    \n",
    "    filename = f'eval_cuML_{method}_{int(datetime.today().timestamp())}'\n",
    "\n",
    "    model = Doc2Vec.load(str(path))\n",
    "    data = model.dv.vectors\n",
    "    dataset = '_'.join(path.stem.split('_')[1:])\n",
    "\n",
    "    print(f'Performing {method} and evaluating for {dataset} points')     \n",
    "\n",
    "    if method=='km':\n",
    "        for n_clusters in range(kwargs['min_clust'], kwargs['max_clust'] + 1):\n",
    "            labels = get_kmeans(data, dataset, distance, n_clusters)\n",
    "            get_results(data, filename, labels, dataset, distance, method, n_clusters=n_clusters)\n",
    "    elif method=='dbscan':\n",
    "        for epsilon in kwargs['eps_range']:\n",
    "            for min_pts in kwargs['min_pts_range']:\n",
    "                labels = get_dbscan(data, dataset, distance, epsilon, min_pts)\n",
    "                get_results(data, filename, labels, dataset, distance, method, epsilon=epsilon, min_pts=min_pts)\n",
    "    elif method=='hdbscan':\n",
    "        for mcs in kwargs['mcs_range']:\n",
    "            for min_samples in kwargs['min_samples_range']:\n",
    "                labels = get_hdbscan(data, dataset, distance, mcs, min_samples)\n",
    "                get_results(data, filename, labels, dataset, distance, method, min_clt_size=mcs, min_samples=min_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8173b59-b6f7-491c-9c63-15f330c3a40f",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a549ba2-5ff6-4440-991b-267c0c57bf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "mcs_range = [mcs for mcs in range(2,21)]\n",
    "print(mcs_range)\n",
    "min_samples_range = [ms for ms in range(2,21)]\n",
    "print(min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb78c9b-ae3c-4a04-8949-4bfba1e93e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for chains_split_2_300 points\n",
      "Took: 0:31:51.766285\n",
      "Performing hdbscan and evaluating for chains_split_2_50 points\n",
      "Took: 0:51:33.349791\n",
      "Performing hdbscan and evaluating for chains_split_3_300 points\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m mod_paths[\u001b[38;5;241m8\u001b[39m:]:\n\u001b[1;32m      4\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mclustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhdbscan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcs_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmcs_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_samples_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;241m-\u001b[39mt0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [3], line 158\u001b[0m, in \u001b[0;36mclustering\u001b[0;34m(path, method, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mcs \u001b[38;5;129;01min\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmcs_range\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m min_samples \u001b[38;5;129;01min\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_range\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 158\u001b[0m         labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_hdbscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m         get_results(data, filename, labels, dataset, distance, method, min_clt_size\u001b[38;5;241m=\u001b[39mmcs, min_samples\u001b[38;5;241m=\u001b[39mmin_samples)\n",
      "Cell \u001b[0;32mIn [3], line 59\u001b[0m, in \u001b[0;36mget_hdbscan\u001b[0;34m(data, dataset, distance, min_clt_size, min_samples)\u001b[0m\n\u001b[1;32m     57\u001b[0m     labels \u001b[38;5;241m=\u001b[39m load_labels(labels_file)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     clusterer \u001b[38;5;241m=\u001b[39m \u001b[43mHDBSCAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_clt_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     labels \u001b[38;5;241m=\u001b[39m clusterer\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m     64\u001b[0m     export_labels(labels, labels_file)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-22.08/lib/python3.9/site-packages/cuml/internals/api_decorators.py:409\u001b[0m, in \u001b[0;36mBaseReturnAnyDecorator.__call__.<locals>.inner_with_setters\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m self_val, input_val, target_val \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_arg_values(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_setters(self_val\u001b[38;5;241m=\u001b[39mself_val,\n\u001b[1;32m    406\u001b[0m                 input_val\u001b[38;5;241m=\u001b[39minput_val,\n\u001b[1;32m    407\u001b[0m                 target_val\u001b[38;5;241m=\u001b[39mtarget_val)\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mhdbscan.pyx:657\u001b[0m, in \u001b[0;36mcuml.cluster.hdbscan.HDBSCAN.fit\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mhdbscan.pyx:535\u001b[0m, in \u001b[0;36mcuml.cluster.hdbscan.HDBSCAN._construct_output_attributes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mhdbscan.pyx:128\u001b[0m, in \u001b[0;36mcuml.cluster.hdbscan._cuml_array_from_ptr\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-22.08/lib/python3.9/site-packages/cuml/common/memory_utils.py:90\u001b[0m, in \u001b[0;36mwith_cupy_rmm.<locals>.cupy_rmm_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__cuml_rmm_wrapped\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcupy_rmm_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cupy_using_allocator(rmm\u001b[38;5;241m.\u001b[39mrmm_cupy_allocator):\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# clustering(mod_paths[:2], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)\n",
    "# for i in range(len(mod_paths)):\n",
    "for path in mod_paths[8:]:\n",
    "    t0 = datetime.now()\n",
    "    clustering(path, method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d8133-7ab6-4675-8832-9e6dcd7a0992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for forward points\n",
      "Took: 0:17:45.370655\n",
      "Performing hdbscan and evaluating for no_duplicates points\n"
     ]
    }
   ],
   "source": [
    "# clustering(mod_paths[2:4], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)\n",
    "for i in range(len(mod_paths[2:3])):\n",
    "    t0 = datetime.now()\n",
    "    clustering(mod_paths[i], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e06d8-c7c9-4fc2-a7ef-270668e53057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for no_duplicates points\n"
     ]
    }
   ],
   "source": [
    "# clustering(mod_paths[3:4], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)\n",
    "for i in range(len(mod_paths[3:4])):\n",
    "    t0 = datetime.now()\n",
    "    clustering(mod_paths[i], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf4a150-8a7c-461f-98ae-3322f79572cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for not_original points\n",
      "Took: 0:59:09.411661\n",
      "Performing hdbscan and evaluating for replies points\n",
      "Took: 1:28:25.749069\n"
     ]
    }
   ],
   "source": [
    "# clustering(mod_paths[4:], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)\n",
    "for i in range(len(mod_paths[4:])):\n",
    "    t0 = datetime.now()\n",
    "    clustering(mod_paths[i], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770db638-c4c9-4f0b-a04d-50a6b4153c10",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2881f0f9-2a66-43a4-8481-e00fae065079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0]\n"
     ]
    }
   ],
   "source": [
    "min_pts_range = [min_pts for min_pts in range(2,16)]\n",
    "print(min_pts_range)\n",
    "eps_range = [round(e*0.01,3) for e in range(20,101)]\n",
    "print(eps_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc644196-89f0-416b-9f45-3ab5acbd36b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_gt_1_300.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_gt_1_50.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_replies_300.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_replies_50.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_split_0_300.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_split_0_50.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_split_1_300.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_split_1_50.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_split_2_300.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_split_2_50.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_split_3_300.model'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/models/d2v_chains_split_3_50.model')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b2bb74-5db0-417f-b9f6-94bb30a01064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dbscan and evaluating for chains_gt_1_300 points\n",
      "Took: 0:06:48.698934\n",
      "Performing dbscan and evaluating for chains_gt_1_50 points\n",
      "Took: 0:03:31.620055\n",
      "Performing dbscan and evaluating for chains_replies_300 points\n",
      "Took: 0:23:15.766989\n",
      "Performing dbscan and evaluating for chains_replies_50 points\n",
      "Took: 0:08:22.964877\n",
      "Performing dbscan and evaluating for chains_split_0_300 points\n",
      "Took: 0:09:07.265597\n",
      "Performing dbscan and evaluating for chains_split_0_50 points\n",
      "Took: 0:04:16.051523\n"
     ]
    }
   ],
   "source": [
    "for path in mod_paths[:6]:\n",
    "    t0 = datetime.now()\n",
    "    clustering(path, method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc608b7-f4ca-4c54-a2d2-3231403df73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mod_paths)):\n",
    "    t0 = datetime.now()\n",
    "    clustering(mod_paths[i], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4a53d2-60aa-4f35-aa65-a4719eeab301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dbscan and evaluating for chain_children points\n",
      "Took: 0:27:42.338594\n",
      "Performing dbscan and evaluating for chain_fathers points\n",
      "Took: 1:01:47.539108\n",
      "Performing dbscan and evaluating for forward points\n",
      "Took: 0:01:20.729172\n"
     ]
    }
   ],
   "source": [
    "# clustering(mod_paths[:3], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)\n",
    "for i in range(len(mod_paths[:3])):\n",
    "    t0 = datetime.now()\n",
    "    clustering(mod_paths[i], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad9979c-6337-4473-92c8-e087feb94987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dbscan and evaluating for not_original points\n",
      "Took: 0:08:55.745006\n",
      "Performing dbscan and evaluating for replies points\n",
      "Took: 0:17:27.458527\n"
     ]
    }
   ],
   "source": [
    "# clustering(mod_paths[4:], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)\n",
    "for i in range(len(mod_paths[4:])):\n",
    "    t0 = datetime.now()\n",
    "    clustering(mod_paths[i], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b84201-5716-4688-8ab0-3401dbc5496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering(mod_paths[3:4], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)\n",
    "for i in range(len(mod_paths[3:4])):\n",
    "    t0 = datetime.now()\n",
    "    clustering(mod_paths[i], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce52f92-3dca-4100-b94a-6d5002f9f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0]\n"
     ]
    }
   ],
   "source": [
    "min_pts_range = [min_pts for min_pts in range(2,16)]\n",
    "print(min_pts_range)\n",
    "eps_range = [round(e*0.01,3) for e in range(10,101)]\n",
    "print(eps_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8398876c-e49e-4765-9b11-8a865d8eca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dbscan and evaluating for chain_children points\n",
      "Took: 0:39:05.931020\n",
      "Performing dbscan and evaluating for chain_fathers points\n",
      "Took: 1:17:36.099185\n",
      "Performing dbscan and evaluating for forward points\n",
      "Took: 0:03:21.913214\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mod_paths[:3])):\n",
    "    t0 = datetime.now()\n",
    "    clustering(mod_paths[i], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range, distance='cosine')\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee39cef6-bcc7-4239-b87c-f2dde3673dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dbscan and evaluating for not_original points\n",
      "Took: 0:21:03.090316\n",
      "Performing dbscan and evaluating for replies points\n",
      "Took: 0:25:28.512184\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mod_paths[4:])):\n",
    "    t0 = datetime.now()\n",
    "    clustering(mod_paths[i], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range, distance='cosine')\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f3735-dafe-4d03-b090-09653fe57a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering(mod_paths[3:4], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range, distance='cosine')\n",
    "for i in range(len(mod_paths[3:4])):\n",
    "    t0 = datetime.now()\n",
    "    clustering(mod_paths[i], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range, distance='cosine')\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56ac51-7cb1-4534-b66c-fcb27d9c07cb",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d0111d-2c79-4435-9c80-1334729c8a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing km and evaluating for chains_gt_1_300 points\n",
      "Took: 0:00:23.300622\n",
      "Performing km and evaluating for chains_gt_1_50 points\n",
      "Took: 0:00:13.242667\n",
      "Performing km and evaluating for chains_replies_300 points\n",
      "Took: 0:00:57.562967\n",
      "Performing km and evaluating for chains_replies_50 points\n",
      "Took: 0:00:37.871400\n",
      "Performing km and evaluating for chains_split_0_300 points\n",
      "Took: 0:00:26.950204\n",
      "Performing km and evaluating for chains_split_0_50 points\n",
      "Took: 0:00:16.945235\n"
     ]
    }
   ],
   "source": [
    "for path in mod_paths[:6]:\n",
    "    t0 = datetime.now()\n",
    "    clustering(path, min_clust=2, max_clust=20)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc6143-610d-4ccb-afaa-001f38dd5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in mod_paths:\n",
    "    t0 = datetime.now()\n",
    "    clustering(path, min_clust=2, max_clust=20)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e0c2515-91e1-46b6-9392-f35781f933b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing km and evaluating for chain_children points\n",
      "Took: 0:00:48.638613\n",
      "Performing km and evaluating for chain_fathers points\n",
      "Took: 0:01:40.050665\n",
      "Performing km and evaluating for forward points\n",
      "Took: 0:00:04.075850\n",
      "Performing km and evaluating for no_duplicates points\n",
      "Took: 0:04:44.697953\n",
      "Performing km and evaluating for not_original points\n",
      "Took: 0:00:19.473320\n",
      "Performing km and evaluating for replies points\n",
      "Took: 0:00:32.727606\n"
     ]
    }
   ],
   "source": [
    "for path in mod_paths:\n",
    "    t0 = datetime.now()\n",
    "    clustering(path, min_clust=2, max_clust=20, distance='cosine')\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took: {t1-t0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd0a079-5144-40a5-9393-43f975d6927a",
   "metadata": {},
   "source": [
    "# HDBSCAN for random sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc633b6-0dbf-4125-b78e-fd0c8df8c044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for 90000 points\n",
      "Took: 0:52:48.133023\n",
      "Performing hdbscan and evaluating for 95000 points\n",
      "Took: 1:13:08.788950\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[-2:], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c576f01-8d99-4e9c-8a83-a7e885988256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "mcs_range = [mcs for mcs in range(2,5)]\n",
    "print(mcs_range)\n",
    "min_samples_range = [ms for ms in range(2,5)]\n",
    "print(min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7f94fc0-fe17-45e6-afe3-588284bbea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for 20000 points\n",
      "Took: 0:00:28.531770\n",
      "Performing hdbscan and evaluating for 25000 points\n",
      "Took: 0:00:34.023335\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[2:4], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332319a5-2ac4-4822-80fa-897095e7b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for 30000 points\n",
      "Took: 0:00:41.591004\n",
      "Performing hdbscan and evaluating for 35000 points\n",
      "Took: 0:00:50.358744\n",
      "Performing hdbscan and evaluating for 40000 points\n",
      "Took: 0:00:58.804751\n",
      "Performing hdbscan and evaluating for 45000 points\n",
      "Took: 0:01:07.808075\n",
      "Performing hdbscan and evaluating for 50000 points\n",
      "Took: 0:01:15.664892\n",
      "Performing hdbscan and evaluating for 55000 points\n",
      "Took: 0:01:28.587734\n",
      "Performing hdbscan and evaluating for 60000 points\n",
      "Took: 0:01:42.591604\n",
      "Performing hdbscan and evaluating for 65000 points\n",
      "Took: 0:01:47.774804\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[4:12], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117c6338-3e2d-4ec7-980b-5a710c1739ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for 70000 points\n",
      "Took: 0:01:58.902803\n",
      "Performing hdbscan and evaluating for 75000 points\n",
      "Took: 0:02:09.473521\n",
      "Performing hdbscan and evaluating for 80000 points\n",
      "Took: 0:02:19.611257\n",
      "Performing hdbscan and evaluating for 85000 points\n",
      "Took: 0:02:26.379817\n",
      "Performing hdbscan and evaluating for 90000 points\n",
      "Took: 0:02:46.450749\n",
      "Performing hdbscan and evaluating for 95000 points\n",
      "Took: 0:02:52.979125\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[12:], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7e26c-c75b-4383-9570-ba0e537e4aa0",
   "metadata": {},
   "source": [
    "# DBSCAN for random sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7a7f9-4b30-4c23-a822-be615c70bea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
