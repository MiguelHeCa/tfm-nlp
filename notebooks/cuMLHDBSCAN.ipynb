{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1cdeb0f-2c13-4659-bb99-e69e819db86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "# from hdbscan import HDBSCAN\n",
    "from cuml.cluster import HDBSCAN, DBSCAN\n",
    "# from sklearn.cluster import DBSCAN, KMeans\n",
    "# from sklearn.decomposition import TruncatedSVD \n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score\n",
    "from cuml.metrics.cluster.entropy import cython_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637ae79a-4a0a-432d-a5ce-b0eaeb3d7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(Path.cwd().parent, 'data/interim')\n",
    "models_dir = Path(Path.cwd().parent, 'models')\n",
    "mod_paths = sorted([str(mod_path) for mod_path in Path(models_dir).glob('*.model')])[1:] # removing 1e5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbd9350-dba3-48d9-b219-0a928b143243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(path, data=None):\n",
    "    if not path.is_file():\n",
    "        with open(path, 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(data.keys())\n",
    "            if data is not None:\n",
    "                writer.writerow(data.values())\n",
    "    else:\n",
    "        with open(path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(data.values())\n",
    "            \n",
    "\n",
    "def load_labels(file):\n",
    "    with open(file, 'rb') as handle:\n",
    "            labels = pkl.load(handle)\n",
    "            \n",
    "    return labels\n",
    "\n",
    "\n",
    "def export_labels(labels, file):\n",
    "    with open(file, 'wb') as handle:\n",
    "        pkl.dump(labels, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# def get_kmeans(data, size, distance, n_clusters):\n",
    "#     labels_file = Path(data_dir, f'labels_km_{size}_{n_clusters:02d}_{distance}.pkl')\n",
    "#     if labels_file.is_file():\n",
    "#         labels = load_labels(labels_file)\n",
    "#     else:\n",
    "#         km = KMeans(n_clusters=n_clusters)\n",
    "#         km.fit(data)\n",
    "#         labels = km.labels_.tolist()\n",
    "#         export_labels(labels, labels_file)\n",
    "        \n",
    "#     return labels\n",
    "\n",
    "\n",
    "def get_dbscan(data, size, distance, epsilon, min_pts):\n",
    "    labels_file = Path(data_dir, f'labels_cuMLdbscan_{size}_{epsilon}_{min_pts:02d}_{distance}.pkl')\n",
    "    if labels_file.is_file():\n",
    "        labels = load_labels(labels_file)\n",
    "    else:\n",
    "        db = DBSCAN(eps=epsilon,\n",
    "                    min_samples=min_pts,\n",
    "                    metric=distance\n",
    "                   ).fit(data)\n",
    "        labels = db.labels_\n",
    "        export_labels(labels, labels_file)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_hdbscan(data, size, distance, min_clt_size, min_samples):\n",
    "    labels_file = Path(data_dir, f'labels_cuMLhdbscan_{size}_{min_clt_size:02d}_{min_samples:02d}_{distance}.pkl')\n",
    "    if labels_file.is_file():\n",
    "        labels = load_labels(labels_file)\n",
    "    else:\n",
    "        clusterer = HDBSCAN(min_cluster_size=min_clt_size,\n",
    "                            min_samples=min_samples,\n",
    "                            metric=distance\n",
    "                           ).fit(data)\n",
    "        labels = clusterer.labels_\n",
    "        export_labels(labels, labels_file)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def evaluate_cluster(data, labels, distance, method, n_clusters=None):\n",
    "    results = {}\n",
    "    \n",
    "    if method != 'km':\n",
    "        count_clust = Counter(labels)\n",
    "        n_clusters = len([key for key in count_clust.keys() if key != -1])\n",
    "        results['n_clusters'] = n_clusters\n",
    "\n",
    "        if -1 in count_clust:\n",
    "            n_noise = count_clust[-1]\n",
    "            results['n_noise'] = n_noise\n",
    "\n",
    "        clust_data = []\n",
    "        clust_labs = []\n",
    "        for i, label in enumerate(labels):\n",
    "            if label != -1:\n",
    "                clust_data.append(data[i])\n",
    "                clust_labs.append(labels[i])\n",
    "    else:\n",
    "        clust_data = data\n",
    "        clust_labs = labels\n",
    "        n_clusters = n_clusters\n",
    "        results['n_clusters'] = n_clusters\n",
    "    \n",
    "    clust_data = np.asarray(clust_data)\n",
    "    clust_labs = np.asarray(clust_labs)\n",
    "    \n",
    "    if len(clust_labs) == n_clusters or n_clusters < 2:\n",
    "        results.update({'sl_score': None, 'ch_score': None, 'db_score': None})\n",
    "    else:\n",
    "        results.update({\n",
    "            # 'sl_score': silhouette_score(clust_data, clust_labs, metric=distance),\n",
    "            'sl_score': cython_silhouette_score(clust_data, clust_labs, metric=distance),\n",
    "            'ch_score': calinski_harabasz_score(clust_data, clust_labs),\n",
    "            'db_score': davies_bouldin_score(clust_data, clust_labs),\n",
    "            'entropy' : cython_entropy(clust_labs)\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(data, filename, labels, size, distance, method, **kwargs):\n",
    "    if method=='km':\n",
    "        n_clusters = kwargs['n_clusters']\n",
    "        cls_res = evaluate_cluster(data, labels, distance, method, n_clusters)\n",
    "    elif method=='dbscan':\n",
    "        cls_res = evaluate_cluster(data, labels, distance, method)\n",
    "        cls_res = {'epsilon': kwargs['epsilon'], 'min_pts': kwargs['min_pts']}  | cls_res\n",
    "    elif method=='hdbscan':\n",
    "        cls_res = evaluate_cluster(data, labels, distance, method)\n",
    "        cls_res = {'min_clt_size': kwargs['min_clt_size'], 'min_samples': kwargs['min_samples']}  | cls_res\n",
    "    \n",
    "    results = {'distance': distance, 'size': size} | cls_res\n",
    "    export_results(Path(data_dir, filename + '.csv'), results)\n",
    "\n",
    "    \n",
    "def clustering(path_list, method='km', **kwargs):\n",
    "    if 'distance' not in kwargs:\n",
    "        distance = 'euclidean'\n",
    "    else:\n",
    "        distance = kwargs['distance']\n",
    "    \n",
    "    filename = f'eval_cuML_{method}_{int(datetime.today().timestamp())}'\n",
    "    \n",
    "    for i in range(len(path_list)):\n",
    "        model = Doc2Vec.load(path_list[i])\n",
    "        data = model.dv.vectors\n",
    "        size = path_list[i].split('_')[2]\n",
    "        \n",
    "        print(f'Performing {method} and evaluating for {size} points')     \n",
    "        \n",
    "        if method=='km':\n",
    "            t0 = datetime.now()\n",
    "            for n_clusters in range(kwargs['min_clust'], kwargs['max_clust'] + 1):\n",
    "                labels = get_kmeans(data, size, distance, n_clusters)\n",
    "                get_results(data, filename, labels, size, distance, method, n_clusters=n_clusters)\n",
    "            t1 = datetime.now()\n",
    "            print(f'Took: {t1-t0}')\n",
    "        elif method=='dbscan':\n",
    "            t0 = datetime.now()\n",
    "            for epsilon in kwargs['eps_range']:\n",
    "                for min_pts in kwargs['min_pts_range']:\n",
    "                    labels = get_dbscan(data, size, distance, epsilon, min_pts)\n",
    "                    get_results(data, filename, labels, size, distance, method, epsilon=epsilon, min_pts=min_pts)\n",
    "            t1 = datetime.now()\n",
    "            print(f'Took: {t1-t0}')\n",
    "        elif method=='hdbscan':\n",
    "            t0 = datetime.now()\n",
    "            for mcs in kwargs['mcs_range']:\n",
    "                for min_samples in kwargs['min_samples_range']:\n",
    "                    labels = get_hdbscan(data, size, distance, mcs, min_samples)\n",
    "                    get_results(data, filename, labels, size, distance, method, min_clt_size=mcs, min_samples=min_samples)\n",
    "            t1 = datetime.now()\n",
    "            print(f'Took: {t1-t0}')\n",
    "                    \n",
    "                    \n",
    "                    # print(f'Calculating clusters with minimum cluster size of {mcs} and {min_samples} minimum samples took: {t1-t0}')\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8173b59-b6f7-491c-9c63-15f330c3a40f",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a549ba2-5ff6-4440-991b-267c0c57bf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "mcs_range = [mcs for mcs in range(5,21)]\n",
    "print(mcs_range)\n",
    "min_samples_range = [ms for ms in range(5,21)]\n",
    "print(min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb78c9b-ae3c-4a04-8949-4bfba1e93e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering(mod_paths[4:], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc633b6-0dbf-4125-b78e-fd0c8df8c044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for 90000 points\n",
      "Took: 0:52:48.133023\n",
      "Performing hdbscan and evaluating for 95000 points\n",
      "Took: 1:13:08.788950\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[-2:], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c576f01-8d99-4e9c-8a83-a7e885988256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "mcs_range = [mcs for mcs in range(2,5)]\n",
    "print(mcs_range)\n",
    "min_samples_range = [ms for ms in range(2,5)]\n",
    "print(min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7f94fc0-fe17-45e6-afe3-588284bbea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for 20000 points\n",
      "Took: 0:00:28.531770\n",
      "Performing hdbscan and evaluating for 25000 points\n",
      "Took: 0:00:34.023335\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[2:4], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332319a5-2ac4-4822-80fa-897095e7b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for 30000 points\n",
      "Took: 0:00:41.591004\n",
      "Performing hdbscan and evaluating for 35000 points\n",
      "Took: 0:00:50.358744\n",
      "Performing hdbscan and evaluating for 40000 points\n",
      "Took: 0:00:58.804751\n",
      "Performing hdbscan and evaluating for 45000 points\n",
      "Took: 0:01:07.808075\n",
      "Performing hdbscan and evaluating for 50000 points\n",
      "Took: 0:01:15.664892\n",
      "Performing hdbscan and evaluating for 55000 points\n",
      "Took: 0:01:28.587734\n",
      "Performing hdbscan and evaluating for 60000 points\n",
      "Took: 0:01:42.591604\n",
      "Performing hdbscan and evaluating for 65000 points\n",
      "Took: 0:01:47.774804\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[4:12], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117c6338-3e2d-4ec7-980b-5a710c1739ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hdbscan and evaluating for 70000 points\n",
      "Took: 0:01:58.902803\n",
      "Performing hdbscan and evaluating for 75000 points\n",
      "Took: 0:02:09.473521\n",
      "Performing hdbscan and evaluating for 80000 points\n",
      "Took: 0:02:19.611257\n",
      "Performing hdbscan and evaluating for 85000 points\n",
      "Took: 0:02:26.379817\n",
      "Performing hdbscan and evaluating for 90000 points\n",
      "Took: 0:02:46.450749\n",
      "Performing hdbscan and evaluating for 95000 points\n",
      "Took: 0:02:52.979125\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[12:], method='hdbscan', mcs_range=mcs_range, min_samples_range=min_samples_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770db638-c4c9-4f0b-a04d-50a6b4153c10",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2881f0f9-2a66-43a4-8481-e00fae065079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9, 10]\n",
      "[0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0]\n"
     ]
    }
   ],
   "source": [
    "min_pts_range = [min_pts for min_pts in range(5,11)]\n",
    "print(min_pts_range)\n",
    "eps_range = [round(e*0.01,3) for e in range(50,101)]\n",
    "print(eps_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc608b7-f4ca-4c54-a2d2-3231403df73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dbscan and evaluating for 10000 points\n",
      "Performing dbscan and evaluating for 15000 points\n",
      "Performing dbscan and evaluating for 20000 points\n",
      "Performing dbscan and evaluating for 25000 points\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[:4], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad9979c-6337-4473-92c8-e087feb94987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dbscan and evaluating for 30000 points\n",
      "Performing dbscan and evaluating for 35000 points\n",
      "Performing dbscan and evaluating for 40000 points\n",
      "Performing dbscan and evaluating for 45000 points\n",
      "Performing dbscan and evaluating for 50000 points\n",
      "Performing dbscan and evaluating for 55000 points\n",
      "Performing dbscan and evaluating for 60000 points\n",
      "Performing dbscan and evaluating for 65000 points\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[4:12], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b84201-5716-4688-8ab0-3401dbc5496a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dbscan and evaluating for 70000 points\n",
      "Performing dbscan and evaluating for 75000 points\n",
      "Performing dbscan and evaluating for 80000 points\n",
      "Performing dbscan and evaluating for 85000 points\n",
      "Performing dbscan and evaluating for 90000 points\n",
      "Performing dbscan and evaluating for 95000 points\n"
     ]
    }
   ],
   "source": [
    "clustering(mod_paths[12:], method='dbscan', eps_range=eps_range, min_pts_range=min_pts_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3dcbe-9664-4b56-bf18-9cca0310b0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
