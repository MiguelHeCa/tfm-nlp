{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4843b6-8ddd-4ea9-afc9-959ac5e35760",
   "metadata": {},
   "source": [
    "# Doc2Vec Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92c8ba-d0b6-47cc-92e7-39b42c008faa",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ddef202-e674-4533-9cdc-79a1a30e181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfee3c-b4ed-48ca-92fb-dce562525725",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d80924c-0aaa-4684-ad83-8c547050cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(Path.cwd().parent, 'data/interim')\n",
    "d2v_dir = Path(data_dir, 'doc2vec')\n",
    "models_dir = Path(Path.cwd().parent, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c3fde4-e47b-4bb1-a5cd-3236ea79c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tagged_docs(data, path, vector_size=300):\n",
    "    file = Path(data_dir, 'tagged_docs_' + str(vector_size) + '_' + '_'.join(path.name.split('_')[2:]))\n",
    "    with open(file, 'wb') as handle:\n",
    "        pkl.dump(data, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "def train_doc2vec(data, path, vector_size=300):\n",
    "    vector_size = vector_size\n",
    "    window_size = 15\n",
    "    min_count = 1\n",
    "    train_epoch = 20\n",
    "    alpha = 0.25\n",
    "    min_alpha = 1e-5\n",
    "    model = Doc2Vec(vector_size=vector_size,\n",
    "                    window=window_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=min_alpha,\n",
    "                    min_count=min_count,\n",
    "                    epochs=train_epoch,\n",
    "                    dm=0)\n",
    "    model.build_vocab(data)\n",
    "    model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save(str(Path(models_dir, 'd2v_' + '_'.join(path.stem.split('_')[2:]) + '_' + str(vector_size) + '.model')))\n",
    "    print('_'.join(path.name.split('_')[2:]) + ' model saved')\n",
    "    \n",
    "\n",
    "def get_doc2vec(path, **kwargs):\n",
    "    df = pd.read_pickle(path)\n",
    "    messages = df['Message'].dropna().to_numpy()\n",
    "    tokenized_docs = [simple_preprocess(msg) for msg in messages]\n",
    "    corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_docs)]\n",
    "    t0 = datetime.now()\n",
    "    if 'vector_size' in kwargs:\n",
    "        save_tagged_docs(corpus, path, kwargs['vector_size'])\n",
    "        train_doc2vec(corpus, path, kwargs['vector_size'])\n",
    "    else:\n",
    "        save_tagged_docs(corpus, path)\n",
    "        train_doc2vec(corpus, path)\n",
    "    t1 = datetime.now()\n",
    "    print(f'Took {t1-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9d5ff-c684-4f82-9c77-86f5d1d6992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for path in sorted(data_dir.glob('parsed_emails*.pkl')):\n",
    "    get_doc2vec(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ddea7-bc8d-474f-b294-1cd8f67bd771",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for path in sorted(data_dir.glob('parsed_emails*.pkl')):\n",
    "    get_doc2vec(path, vector_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2d6aa2-0a17-4734-89b9-d949808ee49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chains_eq_2.pkl model saved\n",
      "Took 0:00:31.677726\n",
      "chains_eq_3.pkl model saved\n",
      "Took 0:00:11.961783\n",
      "chains_ge_10.pkl model saved\n",
      "Took 0:00:05.593118\n",
      "chains_ge_4_lt_10.pkl model saved\n",
      "Took 0:00:12.024035\n",
      "chains_gt_1.pkl model saved\n",
      "Took 0:01:02.423497\n"
     ]
    }
   ],
   "source": [
    "for path in sorted(data_dir.glob('parsed_emails*.pkl'))[1:6]:\n",
    "    get_doc2vec(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef98c7-1d64-4a09-90dc-8c572dda3c26",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda6323-b57e-48b0-9c8e-fd7bb43a4fac",
   "metadata": {},
   "source": [
    "## Assess Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dbebb9-65c5-400f-ad93-c0714238441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "\n",
    "counter = Counter(ranks)\n",
    "print([(i, c, c/len(ranks)*100) for i, c in list(counter.most_common()[:10])])\n",
    "\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(corpus[sims[index][0]].words)))\n",
    "\n",
    "doc_id = random.randint(0, len(corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973d1ddb-1508-4ff6-a528-6083f0922047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# from spacy.tokens import DocBin\n",
    "# from sklearn import tree\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734cbdda-1933-4b27-a75b-664339df6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_trf\") # define your language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "269a073c-f2c0-4d2a-856a-19cfbb1524cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/miguel/Projects/tfm-nlp/data/interim/parsed_emails_chains_all.pkl'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/data/interim/parsed_emails_chains_gt_1.pkl'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/data/interim/parsed_emails_chains_replies.pkl'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/data/interim/parsed_emails_chains_split_0.pkl'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/data/interim/parsed_emails_chains_split_1.pkl'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/data/interim/parsed_emails_chains_split_2.pkl'),\n",
       " PosixPath('/home/miguel/Projects/tfm-nlp/data/interim/parsed_emails_chains_split_3.pkl')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([path for path in data_dir.glob('parsed_emails*.pkl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4defe00e-9ee3-4304-9422-38d6547f611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/home/miguel/Projects/tfm-nlp/data/interim/parsed_emails_chains_gt_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "563682bc-97e6-4900-b4fc-4066dac3b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(string):\n",
    "    doc = nlp(string)\n",
    "    terms = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.is_space\\\n",
    "        and not token.like_url and not token.like_email and not token.is_currency\\\n",
    "        and not token.like_num and token.pos_ != 'X':\n",
    "            terms.append(f'{token}_{token.pos_}_{token.lemma_}')\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55c6d643-6358-4a9b-a655-1fb381b3b433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey_INTJ_hey',\n",
       " 'Paul_PROPN_Paul',\n",
       " 'going_VERB_go',\n",
       " 'Attached_VERB_attach',\n",
       " 'find_VERB_find',\n",
       " 'pics_NOUN_pic',\n",
       " 'halloween_NOUN_halloween',\n",
       " 'party_NOUN_party',\n",
       " 'hope_VERB_hope',\n",
       " 'like_VERB_like',\n",
       " 'going_VERB_go',\n",
       " 'Brasil_PROPN_Brasil',\n",
       " 'today_NOUN_today',\n",
       " 'days_NOUN_day',\n",
       " 'guess_VERB_guess',\n",
       " 'better_ADJ_well',\n",
       " 'Let_VERB_let',\n",
       " 'weekends_NOUN_weekend',\n",
       " 'pretty_ADV_pretty',\n",
       " 'laid_VERB_lay',\n",
       " 'lately_ADV_lately',\n",
       " 'miss_VERB_miss',\n",
       " 'guys_NOUN_guy',\n",
       " 'abraco_PROPN_abraco',\n",
       " 'Eduardo_PROPN_Eduardo']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_terms(df.loc[142, 'Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5812eb4a-d48b-43ef-ac1f-fe7899fffaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(df.loc[142, 'Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "faba3543-b593-4df2-a22a-9d34f9ff4ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_terms \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 2\u001b[0m df_terms[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMessage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_terms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-22.08/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-22.08/lib/python3.9/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-22.08/lib/python3.9/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-22.08/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn [34], line 2\u001b[0m, in \u001b[0;36mget_terms\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_terms\u001b[39m(string):\n\u001b[0;32m----> 2\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     terms \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-22.08/lib/python3.9/site-packages/spacy/language.py:1008\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    989\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    992\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    993\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1008\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1010\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-22.08/lib/python3.9/site-packages/spacy/language.py:1102\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n\u001b[0;32m-> 1102\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE1041\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[0;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'float'>"
     ]
    }
   ],
   "source": [
    "df_terms = {}\n",
    "df_terms['terms'] = df['Message'].apply(get_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8299e7-aa24-4444-89a6-a00f70675975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust attributes to your liking:\n",
    "doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "\n",
    "for doc in nlp.pipe(df['articleDocument'].str.lower()):\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "# either save DocBin to a bytes object, or...\n",
    "#bytes_data = doc_bin.to_bytes()\n",
    "\n",
    "# save DocBin to a file on disc\n",
    "file_name_spacy = 'output/preprocessed_documents.spacy'\n",
    "doc_bin.to_disk(tfi_dir)\n",
    "\n",
    "#Load DocBin at later time or on different system from disc or bytes object\n",
    "#doc_bin = DocBin().from_bytes(bytes_data)\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)\n",
    "\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "print(len(docs))\n",
    "\n",
    "tokenized_lemmatized_texts = [[token.lemma_ for token in doc \n",
    "                               if not token.is_stop and not token.is_punct and not token.is_space and not token.like_url and not token.like_email] \n",
    "                               for doc in docs]\n",
    "\n",
    "# classifier to use\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# just some random target response\n",
    "y = np.random.randint(2, size=len(docs))\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x: x, max_features=3000)\n",
    "\n",
    "pipeline = Pipeline([('vect', vectorizer), ('dectree', clf)])\n",
    "parameters = {'dectree__max_depth':[4, 10]}\n",
    "gs_clf = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "gs_clf.fit(tokenized_lemmatized_texts, y)\n",
    "print(gs_clf.best_estimator_.get_params()['dectree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c685537-8ae6-4ec6-b92b-b0873d7603a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust attributes to your liking:\n",
    "doc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\n",
    "\n",
    "for doc in nlp.pipe(df['articleDocument'].str.lower()):\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "# either save DocBin to a bytes object, or...\n",
    "#bytes_data = doc_bin.to_bytes()\n",
    "\n",
    "# save DocBin to a file on disc\n",
    "file_name_spacy = 'output/preprocessed_documents.spacy'\n",
    "doc_bin.to_disk(tfi_dir)\n",
    "\n",
    "#Load DocBin at later time or on different system from disc or bytes object\n",
    "#doc_bin = DocBin().from_bytes(bytes_data)\n",
    "doc_bin = DocBin().from_disk(file_name_spacy)\n",
    "\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "print(len(docs))\n",
    "\n",
    "tokenized_lemmatized_texts = [[token.lemma_ for token in doc \n",
    "                               if not token.is_stop and not token.is_punct and not token.is_space and not token.like_url and not token.like_email] \n",
    "                               for doc in docs]\n",
    "\n",
    "# classifier to use\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# just some random target response\n",
    "y = np.random.randint(2, size=len(docs))\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, tokenizer=lambda x: x, max_features=3000)\n",
    "\n",
    "pipeline = Pipeline([('vect', vectorizer), ('dectree', clf)])\n",
    "parameters = {'dectree__max_depth':[4, 10]}\n",
    "gs_clf = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "gs_clf.fit(tokenized_lemmatized_texts, y)\n",
    "print(gs_clf.best_estimator_.get_params()['dectree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3b05c-6401-480e-a731-83cbd96963ab",
   "metadata": {},
   "source": [
    "## Direct TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efdf1170-f426-4f92-b87c-315952a2a77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/miguel/Projects/tfm-nlp/data/interim/tfidf')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfi_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f73af-c99f-4ff2-bef4-88b5f0136146",
   "metadata": {},
   "source": [
    "## Modified TFIDF with vector norm, lemma and pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547bc01-14f0-4590-b0b4-af2458617361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
