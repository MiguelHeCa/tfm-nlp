{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86965aa6-ef5a-4ea6-b40a-71fe67039cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from dateutil.parser import parse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78db7a1f-244d-40d9-9a1f-5dc57e57f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_dir = Path(Path.cwd().parent, Path('data/raw/maildir'))\n",
    "\n",
    "clean_emails = []\n",
    "for path in email_dir.rglob('*.'):\n",
    "    if 'all_documents' not in str(path.parent) and 'discussion_threads' not in str(path.parent):\n",
    "        clean_emails.append(path)\n",
    "\n",
    "\n",
    "def parse_emails(path):\n",
    "    with open(path, 'r', encoding='windows-1252') as f:\n",
    "        parsed_email = email.message_from_file(f)\n",
    "    return parsed_email\n",
    "\n",
    "\n",
    "#check to see if an email is part of chain by looking for the '-----Original Message-----' tag\n",
    "#addtionally count the number of times the tag appears to get the depth of the chain\n",
    "def get_chain(payload):\n",
    "    chain_count = 0\n",
    "    is_chain = False\n",
    "    check_value = '-----Original Message-----'\n",
    "    if check_value in payload:\n",
    "        chain_count = payload.count(check_value)\n",
    "        is_chain = True\n",
    "    return chain_count, is_chain\n",
    "\n",
    "\n",
    "def get_parsed_emails(paths):\n",
    "    emails = []\n",
    "    for i, path in enumerate(paths):\n",
    "        eml = parse_emails(path)\n",
    "        tms = int(parse(eml['Date']).timestamp())\n",
    "        chain_count, is_chain = get_chain(eml.get_payload())\n",
    "        emails.append((i, eml, tms, chain_count, is_chain))\n",
    "    return emails\n",
    "\n",
    "\n",
    "parsed_emails = get_parsed_emails(clean_emails)\n",
    "\n",
    "# lengths = [(text[0], text[1]['message-id'],len(text[1].get_payload())) for text in parsed_emails]\n",
    "# large_emails = sorted(lengths, key=lambda l: l[2], reverse=True)[:10]\n",
    "# # large_emails\n",
    "# l_ids = list(zip(*large_emails))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4634fb3e-a503-4763-a794-0b2d103f2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_emails = [email for email in parsed_emails if email[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e8f49ee-b799-410b-a8db-919b5d2925a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65075"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chain_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54cbc1a-ef23-434c-8fd9-c6a1abc37243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7842be8-44d1-4e67-9f82-2e6a8185dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in l_ids:\n",
    "#     print(clean_emails[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ca44eb-ed6f-435b-92d5-c48f3696ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = [(text[0], text[1]['message-id'], len(preprocess_message(text[1].get_payload()))) for text in parsed_emails]\n",
    "# large_emails = sorted(lengths, key=lambda l: l[2], reverse=True)[:10]\n",
    "# large_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa312891-b4e8-4d16-a47e-c6bf7370fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#check to see if an email is forwarded by looking for 'fw' and 'fwd' tags\n",
    "def is_forwarded(email):\n",
    "    is_forwarded = False\n",
    "    check_values = ['fw:', 'fwd:']\n",
    "    for value in check_values:\n",
    "        if value in email.get_payload().lower() or value in email['Subject'].lower():\n",
    "            is_forwarded = True\n",
    "    return is_forwarded\n",
    "\n",
    "\n",
    "#get only the names from the x-headers in the e-mail\n",
    "def parseXHeaders(header):\n",
    "    if header is not None:\n",
    "        return re.sub(r'(<.*?>,|<.*?>)', '|', header)[:-1]\n",
    "    else:\n",
    "        return header \n",
    "\n",
    "\n",
    "def get_emails_list(dir_path):\n",
    "    clean_emails = []\n",
    "    for path in dir_path.rglob('*.'):\n",
    "        if 'all_documents' not in str(path.parent) and 'discussion_threads' not in str(path.parent):\n",
    "            clean_emails.append(path)\n",
    "    return clean_emails\n",
    "\n",
    "\n",
    "def parse_emails(path):\n",
    "    with open(path, 'r', encoding='windows-1252') as f:\n",
    "        parsed_email = email.message_from_file(f)\n",
    "    return parsed_email\n",
    "\n",
    "\n",
    "def load_emails(dir_path):\n",
    "    parsed_emails = []\n",
    "    clean_emails = get_emails_list(dir_path)\n",
    "    for i, path in enumerate(clean_emails):\n",
    "        parsed_emails.append((i,parse_emails(path)))\n",
    "    return parsed_emails\n",
    "\n",
    "\n",
    "def preprocess_message(text):\n",
    "    msg_end_pattern = re.compile('_{4,}.*|\\n{3,}|<[^>]*>|-{4,}(.*)(\\d{2}:\\d{2}:\\d{2})\\s*(PM|AM)', re.MULTILINE)\n",
    "    try:\n",
    "        msg_end_iter = msg_end_pattern.search(text).start()\n",
    "        # print('end of line:', msg_end_iter)\n",
    "        message = text[:msg_end_iter]\n",
    "    except AttributeError: # not a reply\n",
    "        message = text\n",
    "    return message\n",
    "\n",
    "\n",
    "def remove_spaces(string):\n",
    "    if string is not None:        \n",
    "        string = re.sub('\\s+', ' ', string)\n",
    "        string = string.split(', ')\n",
    "    return string\n",
    "\n",
    "\n",
    "#\n",
    "# Function: get_or_allocated_uid\n",
    "# Arguments: name - string of a user email\n",
    "# Returns: unique integer id\n",
    "#\n",
    "def get_or_allocate_uid(name):\n",
    "     if name not in users:\n",
    "         users[name] = len(users)\n",
    "     return users[name]\n",
    "\n",
    "#\n",
    "# Function: get_or_allocate_tid\n",
    "# Arguments: name - string of email subject line\n",
    "# Returns: unique integer id\n",
    "#\n",
    "def get_or_allocate_tid(name):\n",
    "    parsed_name = re.sub(\"(RE|Re|FWD|Fwd): \", \"\", name)\n",
    "    if parsed_name not in threads:\n",
    "        threads[parsed_name] = len(threads)\n",
    "    return threads[parsed_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05291c16-1ef9-4c6e-bacb-c036149c6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [email[1]['Date'] for email in parsed_emails]\n",
    "subjects = [email[1]['Subject'] for email in parsed_emails]\n",
    "recipients = [remove_spaces(email[1]['To']) for email in parsed_emails]\n",
    "senders = [email[1]['From'] for email in parsed_emails]\n",
    "ccs = [remove_spaces(email[1]['cc']) for email in parsed_emails]\n",
    "bccs = [remove_spaces(email[1]['bcc']) for email in parsed_emails]\n",
    "idx = [email[0] for email in parsed_emails]\n",
    "timestamps = [email[2] for email in parsed_emails]\n",
    "# body = [email[1].get_payload() for email in parsed_emails]\n",
    "fwds = [is_forwarded(email[1]) for email in parsed_emails]\n",
    "chain_count, is_chain = zip(*[is_chain(email[1].get_payload()) for email in parsed_emails])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "923a192f-8a26-4660-b5ae-3c9d1154e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a87b9c5-ca94-44a1-87af-eae9af69f7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39779"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(fwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1715718e-1765-45f4-a01e-df0cf4fb51ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37717311431586775"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(chain_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9de88999-f82f-4d0b-844d-3386eb808d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65075"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(is_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3cd4ed-51ab-44f9-b9f9-d81f12f365d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for email in sel_emails[:5]:\n",
    "    print('Date: ', email[1]['Date'])\n",
    "    print('Subject: ', email[1]['Subject'])\n",
    "    print('Sender: ', email[1]['From'])\n",
    "    print('Receiver: ', email[1]['To'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22266605-6f52-4d1c-8ec7-047b48c36d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = [email[1]['X-Origin'] for email in sel_emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91c28d-5284-4576-b5d6-a4b4226caa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_emails[0][1].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd4b66-5d3c-418d-973e-b1db71495474",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(origins).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2a1aa-d381-4f19-89e4-fa6bc56f37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [email[1]['Date'] for email in parsed_emails]\n",
    "subjects = [email[1]['Subject'] for email in parsed_emails]\n",
    "recipients = [remove_spaces(email[1]['To']) for email in parsed_emails]\n",
    "senders = [email[1]['From'] for email in parsed_emails]\n",
    "ccs = [remove_spaces(email[1]['cc']) for email in parsed_emails]\n",
    "bccs = [remove_spaces(email[1]['bcc']) for email in parsed_emails]\n",
    "idx = [email[0] for email in parsed_emails]\n",
    "timestamps = [email[2] for email in parsed_emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf8a9c-3f1c-43c6-a1cd-1819e9456797",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [preprocess_message(text[1].get_payload()) for text in parsed_emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d2946-a5df-4cf5-ae8a-6a850cdb2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds = []\n",
    "users = {}\n",
    "threads = {}\n",
    "thread_users = {}\n",
    "user_threads = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5244d-115a-422b-9ac6-d02f6fcbac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_emails[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ad249-f1cf-48ac-af59-ca205edd47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(parse(time[0]).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea846ce3-4500-4ad1-856b-6a631414955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(timestamps).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf94b5-90db-493e-8182-c461fe3483ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = [email[1]['X-Origin'] for email in sel_emails]\n",
    "Counter(origins).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c4304-86ce-42dd-a419-26abc1e4ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = {}\n",
    "thread_id = []\n",
    "for sbj in subject:\n",
    "    thread_id.append(get_or_allocate_tid(sbj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d174ad8-08ab-4992-9eeb-ec02558dc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca431f3-7244-4a0d-bc68-6073667479e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbaa606-b413-47d6-9bb1-5af36b9c600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [i for i in timestamps if i == 993682920]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd00c3a-3ce0-4443-8d58-2ead44cf3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a46d8-10a5-44a8-8958-cba9f5a1150c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ign_dir = sorted(sorted(email_dir.glob('**/discussion_threads')) + sorted(email_dir.glob('**/all_documents')))\n",
    "# emails = sorted(email_dir.rglob(\"*.\"))\n",
    "# len(emails)\n",
    "# clean_emails = [path for path in email_dir.rglob('*.') if 'discussion_threads' not in str(path.parent)]\n",
    "# len(clean_emails)\n",
    "# len(emails) - len(clean_emails)\n",
    "# clean_emails = [path for path in email_dir.rglob('*.') if 'all_documents' not in str(path.parent)]\n",
    "# len(clean_emails)\n",
    "# len(emails) - len(clean_emails)\n",
    "# len(parsed_emails)\n",
    "# parsed_emails[0][1].items()\n",
    "\n",
    "# parsed_emails[0][1]\n",
    "# clean_emails[0]\n",
    "# parsed_emails[0][1]['X-Origin']\n",
    "# origins = [email[1]['X-Origin'] for email in parsed_emails]\n",
    "# origins = []\n",
    "# for email in parsed_emails:\n",
    "#     origin = email[1]['X-Origin']\n",
    "#     if origin is not None:\n",
    "#         origins.append(origin.lower())\n",
    "#     else:\n",
    "#         origins.append(origin)\n",
    "# print(len(set(origins)))\n",
    "# Counter(origins).most_common()[:10]\n",
    "# parsed_emails[1][1]\n",
    "# senders = []\n",
    "# for email in parsed_emails:\n",
    "#     sender = email[1]['From']\n",
    "#     senders.append(sender)\n",
    "# Counter(senders).most_common()[:10]\n",
    "# subjects = []\n",
    "# for email in parsed_emails:\n",
    "#     subject = email[1]['Subject']\n",
    "#     subjects.append(subject)\n",
    "# Counter(subjects).most_common()[:20]\n",
    "\n",
    "# Precompiled patterns for performance\n",
    "#\n",
    "# time_pattern = re.compile(\"Date: (?P<data>[A-Z][a-z]+\\, \\d{1,2} [A-Z][a-z]+ \\d{4} \\d{2}\\:\\d{2}\\:\\d{2} \\-\\d{4} \\([A-Z]{3}\\))\")\n",
    "# subject_pattern = re.compile(\"Subject: (?P<data>.*)\")\n",
    "# sender_pattern = re.compile(\"From: (?P<data>.*)\")\n",
    "# recipient_pattern = re.compile(\"To: (?P<data>.*)\")\n",
    "# cc_pattern = re.compile(\"cc: (?P<data>.*)\")\n",
    "# bcc_pattern = re.compile(\"bcc: (?P<data>.*)\")\n",
    "# msg_start_pattern = re.compile(\"\\n\\n\", re.MULTILINE)\n",
    "# msg_end_pattern = re.compile(\"\\n+.*\\n\\d+/\\d+/\\d+ \\d+:\\d+ [AP]M\", re.MULTILINE)\n",
    "# re.compile('-{4,}.*|-{4,}(.*)(\\d{2}:\\d{2}:\\d{2})\\s*(PM|AM)', re.MULTILINE)\n",
    "\n",
    "# text = sample_raw_email.replace(\"\\r\", \"\")\n",
    "# time = time_pattern.search(text).group(\"data\").replace(\"\\n\", \"\")\n",
    "# print(time)\n",
    "# print(sample_email['Date'])\n",
    "# subject = subject_pattern.search(text).group(\"data\").replace(\"\\n\", \"\")\n",
    "# print(subject)\n",
    "# print(sample_email['Subject'])\n",
    "# sender = sender_pattern.search(text).group(\"data\").replace(\"\\n\", \"\")\n",
    "# print(sender)\n",
    "# print(sample_email['From'])\n",
    "# recipient = recipient_pattern.search(text).group(\"data\").split(\", \")\n",
    "# print(recipient)\n",
    "# print(sample_email['To'].split(','))\n",
    "\n",
    "# # cc = cc_pattern.search(text).group(\"data\").split(\", \")\n",
    "# # bcc = bcc_pattern.search(text).group(\"data\").split(\", \")\n",
    "\n",
    "# msg_start_iter = msg_start_pattern.search(text).end()\n",
    "# try:\n",
    "#     msg_end_iter = msg_end_pattern.search(text).start()\n",
    "#     message = text[msg_start_iter:msg_end_iter]\n",
    "# except AttributeError: # not a reply\n",
    "#     message = text[msg_start_iter:]\n",
    "    \n",
    "# message = re.sub(\"[\\n\\r]\", \" \", message)\n",
    "# print(message)\n",
    "# message = re.sub(\"  +\", \" \", message)\n",
    "# print(message)\n",
    "\n",
    "# msg_end_pattern = re.compile('-{4,}.*|[-_]{4,}(.*)(\\d{2}:\\d{2}:\\d{2})\\s*(PM|AM)', re.MULTILINE)\n",
    "# # re.compile('-{4,}.*|-{4,}(.*)(\\d{2}:\\d{2}:\\d{2})\\s*(PM|AM)', re.MULTILINE)\n",
    "# text2 = raw_emails[258598][1]\n",
    "# # print(text2)\n",
    "# msg_start_iter = msg_start_pattern.search(text2).end()\n",
    "# try:\n",
    "#     msg_end_iter = msg_end_pattern.search(text2).start()\n",
    "#     message = text2[msg_start_iter:msg_end_iter]\n",
    "# except AttributeError: # not a reply\n",
    "#     message = text2[msg_start_iter:3000]\n",
    "\n",
    "# print(parsed_emails[249493][1].get_payload())\n",
    "\n",
    "# re.compile('_{4,}.*|\\n{3,}|<[^>]*>|-{4,}(.*)(\\d{2}:\\d{2}:\\d{2})\\s*(PM|AM)', re.MULTILINE).search(parsed_emails[large_emails[0][0]][1].get_payload()).start()\n",
    "# re.compile('_{4,}.*', re.MULTILINE).search(parsed_emails[large_emails[6][0]][1].get_payload()).start()\n",
    "# re.compile('_{4,}.*|\\n{3,}|<[^>]*>|-{4,}(.*)(\\d{2}:\\d{2}:\\d{2})\\s*(PM|AM)', re.MULTILINE).search(parsed_emails[large_emails[6][0]][1].get_payload()).start()\n",
    "\n",
    "# time_pattern = re.compile(\"Date: (?P<data>[A-Z][a-z]+\\, \\d{1,2} [A-Z][a-z]+ \\d{4} \\d{2}\\:\\d{2}\\:\\d{2} \\-\\d{4} \\([A-Z]{3}\\))\")\n",
    "# subject_pattern = re.compile(\"Subject: (?P<data>.*)\")\n",
    "# sender_pattern = re.compile(\"From: (?P<data>.*)\")\n",
    "# recipient_pattern = re.compile(\"To: (?P<data>.*)\")\n",
    "# cc_pattern = re.compile(\"cc: (?P<data>.*)\")\n",
    "# bcc_pattern = re.compile(\"bcc: (?P<data>.*)\")\n",
    "# msg_start_pattern = re.compile(\"\\n\\n\", re.MULTILINE)\n",
    "# msg_end_pattern = re.compile(\"\\n+.*\\n\\d+/\\d+/\\d+ \\d+:\\d+ [AP]M\", re.MULTILINE)\n",
    "#\n",
    "# Function: get_or_allocated_uid\n",
    "# Arguments: name - string of a user email\n",
    "# Returns: unique integer id\n",
    "#\n",
    "# def get_or_allocate_uid(name):\n",
    "#      if name not in users:\n",
    "#          users[name] = len(users)\n",
    "#      return users[name]\n",
    "\n",
    "#\n",
    "# Function: get_or_allocate_tid\n",
    "# Arguments: name - string of email subject line\n",
    "# Returns: unique integer id\n",
    "#\n",
    "# def get_or_allocate_tid(name):\n",
    "#     parsed_name = re.sub(\"(RE|Re|FWD|Fwd): \", \"\", name)\n",
    "#     if parsed_name not in threads:\n",
    "#         threads[parsed_name] = len(threads)\n",
    "#     return threads[parsed_name]\n",
    "\n",
    "# sender_id = get_or_allocate_uid(sender)\n",
    "# recipient_id = [get_or_allocate_uid(u.replace(\"\\n\", \"\")) for u in recipient if u!=\"\"]\n",
    "# cc_ids = [get_or_allocate_uid(u.replace(\"\\n\", \"\")) for u in cc if u!=\"\"]\n",
    "# bcc_ids = [get_or_allocate_uid(u.replace(\"\\n\", \"\")) for u in bcc if u!=\"\"]\n",
    "# thread_id = get_or_allocate_tid(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3debc4-959f-4c3b-8c44-7783c644e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(pathname, orig=True):\n",
    "    if path.isdir(pathname):\n",
    "        print(pathname)\n",
    "        emails = []\n",
    "        for child in listdir(pathname):\n",
    "            # only parse visible files\n",
    "            if child[0] != \".\":\n",
    "                parse_email(path.join(pathname, child), False)\n",
    "    else:\n",
    "        print(\"file %s\" % pathname)\n",
    "        with open(pathname) as TextFile:\n",
    "            text = TextFile.read().replace(\"\\r\", \"\")\n",
    "            try:\n",
    "                time = time_pattern.search(text).group(\"data\").replace(\"\\n\", \"\")\n",
    "                subject = subject_pattern.search(text).group(\"data\").replace(\"\\n\", \"\")\n",
    "\n",
    "                sender = sender_pattern.search(text).group(\"data\").replace(\"\\n\", \"\")\n",
    "\n",
    "                recipient = recipient_pattern.search(text).group(\"data\").split(\", \")\n",
    "                cc = cc_pattern.search(text).group(\"data\").split(\", \")\n",
    "                bcc = bcc_pattern.search(text).group(\"data\").split(\", \")\n",
    "                msg_start_iter = msg_start_pattern.search(text).end()\n",
    "                try:\n",
    "                    msg_end_iter = msg_end_pattern.search(text).start()\n",
    "                    message = text[msg_start_iter:msg_end_iter]\n",
    "                except AttributeError: # not a reply\n",
    "                    message = text[msg_start_iter:]\n",
    "                message = re.sub(\"[\\n\\r]\", \" \", message)\n",
    "                message = re.sub(\"  +\", \" \", message)\n",
    "            except AttributeError:\n",
    "                logging.error(\"Failed to parse %s\" % pathname) \n",
    "                return None\n",
    "            # get user and thread ids\n",
    "            sender_id = get_or_allocate_uid(sender)\n",
    "            recipient_id = [get_or_allocate_uid(u.replace(\"\\n\", \"\")) for u in recipient if u!=\"\"]\n",
    "            cc_ids = [get_or_allocate_uid(u.replace(\"\\n\", \"\")) for u in cc if u!=\"\"]\n",
    "            bcc_ids = [get_or_allocate_uid(u.replace(\"\\n\", \"\")) for u in bcc if u!=\"\"]\n",
    "            thread_id = get_or_allocate_tid(subject)\n",
    "        if thread_id not in thread_users:\n",
    "            thread_users[thread_id] = set()\n",
    "        # maintain list of users involved in thread\n",
    "        users_involved = []\n",
    "        users_involved.append(sender_id)\n",
    "        users_involved.extend(recipient_id)\n",
    "        users_involved.extend(cc_ids)\n",
    "        users_involved.extend(bcc_ids)\n",
    "        thread_users[thread_id] |= set(users_involved)\n",
    "        # maintain list of threads where user is involved\n",
    "        for user in set(users_involved):\n",
    "            if user not in user_threads:\n",
    "                user_threads[user] = set()\n",
    "            user_threads[user].add(thread_id)\n",
    " \n",
    "        entry =  {\"time\": time, \"thread\": thread_id, \"sender\": sender_id, \"recipient\": recipient_id, \"cc\": cc_ids, \"bcc\": bcc_ids, \"message\": message}\n",
    "        feeds.append(entry)\n",
    "    if orig:\n",
    "        try:\n",
    "            with open('messages.json', 'w') as f:\n",
    "                json.dump(feeds, f)\n",
    "            with open('users.json', 'w') as f:\n",
    "                json.dump(users, f)\n",
    "            with open('threads.json', 'w') as f:\n",
    "                json.dump(threads, f)\n",
    "            with open('thread-users.json', 'w') as f:\n",
    "                for thread in thread_users:\n",
    "                    thread_users[thread] = list(thread_users[thread])\n",
    "                json.dump(thread_users, f)\n",
    "            with open('user-threads.json', 'w') as f:\n",
    "                for user in user_threads:\n",
    "                    user_threads[user] = list(user_threads[user])\n",
    "                json.dump(user_threads, f)\n",
    "        except IOError:\n",
    "            print(\"Unable to write to output files, aborting\")\n",
    "            exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0e2e0-ff25-4496-8fe1-beaf23f2938c",
   "metadata": {},
   "source": [
    "# Doc2Vec tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f342d9b1-e7a4-4c85-9d78-9ff13ca2e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68eeaf76-6da4-4344-9953-2f77876983a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(Path.cwd().parent, 'data/interim')\n",
    "models_dir = Path(Path.cwd().parent, 'src/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d4944b-6a8a-4723-8dc6-9f907a97a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(data_dir, 'message.pkl'), 'rb') as handle:\n",
    "    messages = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2708549-98c5-46df-bf2a-f585aabe1148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330689"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b048625f-47bf-4833-9f50-c6ac6e29679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_3 = [msg for i, msg, t in messages if not str.isspace(msg) or msg != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b17a5a4-2381-457a-a717-d78b9c9257e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails(size=int(1e4), seed=42):\n",
    "    with open(Path(data_dir, 'message.pkl'), 'rb') as handle:\n",
    "          messages = pkl.load(handle)\n",
    "    \n",
    "    filtered_emails = [(i, msg) for i, msg, t in messages if not str.isspace(msg) or msg != '']\n",
    "    random.seed(seed)\n",
    "    emails = random.sample(filtered_emails, k=size)\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "881daa73-15d5-40ee-8f6a-5fe8c1d8e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = load_emails()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8666d162-d8f9-4f08-b2c8-61e8bb14b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_documents = []\n",
    "\n",
    "# for i in range(len(emails)):\n",
    "#   tokenized_documents.append(nltk.word_tokenize(emails[i]))\n",
    "\n",
    "# tkn_documents = [nltk.word_tokenize]\n",
    "\n",
    "tokenized_docs = [simple_preprocess(emails[i][1]) for i in range(len(emails))]\n",
    "corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_docs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "438a2c68-e87d-4292-94fd-df3594d725b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "train_epoch = 20\n",
    "alpha = 0.25\n",
    "min_alpha = 1e-5\n",
    "model = Doc2Vec(vector_size=vector_size,\n",
    "                window=window_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=min_alpha,\n",
    "                min_count=min_count,\n",
    "                epochs=train_epoch,\n",
    "                dm=0)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eed31b63-6ee5-4237-875b-b6739c20deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 864, 8.64), (1, 156, 1.5599999999999998), (2, 84, 0.84), (3, 66, 0.66), (4, 45, 0.44999999999999996), (5, 38, 0.38), (8, 34, 0.33999999999999997), (7, 32, 0.32), (10, 29, 0.29), (6, 27, 0.27)]\n",
      "Document (9999): «thanks for update pls keep sending info»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dbow,d300,n5,s0.001,t3>:\n",
      "\n",
      "MOST (5972, 0.6624950766563416): «please let me know if you have any questions thanks fran»\n",
      "\n",
      "SECOND-MOST (8444, 0.6498376727104187): «thanks tim»\n",
      "\n",
      "MEDIAN (6174, 0.26545777916908264): «please review the home addresss and phone number shown on this email do not respond if the information is correct however if incorrect then reply to this email with the correct information so you hr records can be updated accordingly your home information is important in case the company needs to forward critical materials to you or contact you regarding important company information»\n",
      "\n",
      "LEAST (7326, -0.2550029456615448): «forwarded by lisa yoho na enron on am»\n",
      "\n",
      "Train Document (7486): «here is the uae model ben»\n",
      "\n",
      "Similar Document (9710, 0.5885195732116699): «here draft for your review we should talk about the generator before this is sent to mpc»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "\n",
    "counter = Counter(ranks)\n",
    "print([(i, c, c/len(ranks)*100) for i, c in list(counter.most_common()[:10])])\n",
    "\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(corpus[sims[index][0]].words)))\n",
    "\n",
    "doc_id = random.randint(0, len(corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1185e9-b6cf-462f-9061-3bf864abd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "window_size = 10\n",
    "min_count = 1\n",
    "train_epoch = 20\n",
    "alpha = 0.25\n",
    "min_alpha = 1e-4\n",
    "model = Doc2Vec(vector_size=vector_size,\n",
    "                window=window_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=min_alpha,\n",
    "                min_count=min_count,\n",
    "                epochs=train_epoch,\n",
    "                dm=0)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "counter = Counter(ranks)\n",
    "print([(i, c, c/len(ranks)*100) for i, c in list(counter.most_common()[:10])])\n",
    "\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(corpus[sims[index][0]].words)))\n",
    "\n",
    "doc_id = random.randint(0, len(corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2b9db-593d-4c6b-a826-b80a3828eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "train_epoch = 50\n",
    "alpha = 0.25\n",
    "min_alpha = 1e-4\n",
    "model = Doc2Vec(vector_size=vector_size,\n",
    "                window=window_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=min_alpha,\n",
    "                min_count=min_count,\n",
    "                epochs=train_epoch,\n",
    "                dm=0)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "counter = Counter(ranks)\n",
    "print([(i, c, c/len(ranks)*100) for i, c in list(counter.most_common()[:10])])\n",
    "\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(corpus[sims[index][0]].words)))\n",
    "\n",
    "doc_id = random.randint(0, len(corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77c774-5ca4-4622-81b9-a4d1c0879a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "train_epoch = 20\n",
    "alpha = 0.25\n",
    "min_alpha = 1e-10\n",
    "model = Doc2Vec(vector_size=vector_size,\n",
    "                window=window_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=min_alpha,\n",
    "                min_count=min_count,\n",
    "                epochs=train_epoch,\n",
    "                dm=0)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "counter = Counter(ranks)\n",
    "print([(i, c, c/len(ranks)*100) for i, c in list(counter.most_common()[:10])])\n",
    "\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(corpus[sims[index][0]].words)))\n",
    "\n",
    "doc_id = random.randint(0, len(corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71349f3-eac8-4ab1-ac1e-6ae8aa655520",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 300\n",
    "window_size = 20\n",
    "min_count = 1\n",
    "train_epoch = 20\n",
    "alpha = 0.25\n",
    "min_alpha = 1e-5\n",
    "model = Doc2Vec(vector_size=vector_size,\n",
    "                window=window_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=min_alpha,\n",
    "                min_count=min_count,\n",
    "                epochs=train_epoch,\n",
    "                dm=0)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "counter = Counter(ranks)\n",
    "print([(i, c, c/len(ranks)*100) for i, c in list(counter.most_common()[:10])])\n",
    "\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(corpus[sims[index][0]].words)))\n",
    "\n",
    "doc_id = random.randint(0, len(corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cca2c-02fa-4d3e-8435-a260038c5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 2\n",
    "train_epoch = 20\n",
    "alpha = 0.25\n",
    "min_alpha = 1e-5\n",
    "model = Doc2Vec(vector_size=vector_size,\n",
    "                window=window_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=min_alpha,\n",
    "                min_count=min_count,\n",
    "                epochs=train_epoch,\n",
    "                dm=0)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "counter = Counter(ranks)\n",
    "print([(i, c, c/len(ranks)*100) for i, c in list(counter.most_common()[:10])])\n",
    "\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(corpus[sims[index][0]].words)))\n",
    "\n",
    "doc_id = random.randint(0, len(corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a9d6a-55b5-4874-8f04-7d141f66544a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
