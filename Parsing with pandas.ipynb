{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce47a64",
   "metadata": {},
   "source": [
    "# Converting parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bcec8fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wr/t46kdpfx1578xc_8wrsrr4cw0000gn/T/ipykernel_5306/4100369177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyfreeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import email\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import pyfreeling\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from dateutil import parser\n",
    "from email_reply_parser import EmailReplyParser\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec570c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_emails(dir_list, **kwargs):\n",
    "    df_dict = {}\n",
    "    \n",
    "    df_dict['id'] = []\n",
    "    mg_list = []\n",
    "    for mail in dir_list:\n",
    "        df_dict['id'].append(get_email_id(mail))\n",
    "        mg_list.append(email_parser(mail))\n",
    "    \n",
    "    if 'From' in kwargs:\n",
    "        df_dict['from'] = []\n",
    "        \n",
    "    if 'To' in kwargs:\n",
    "        df_dict['to'] = []\n",
    "    \n",
    "    if 'Date' in kwargs:\n",
    "        df_dict['date'] = []\n",
    "        \n",
    "    df_dict['text'] = []\n",
    "    \n",
    "    for mg in mg_list:\n",
    "        if 'from' in df_dict:\n",
    "            df_dict['from'].append(mg['From'])\n",
    "        if 'to' in df_dict:\n",
    "            df_dict['to'].append(preprocess_recipients(mg['To']))\n",
    "        if 'date' in df_dict:\n",
    "            df_dict['date'].append(get_timestamp(mg['Date']))\n",
    "        df_dict['text'].append(EmailReplyParser.parse_reply(mg.get_payload()))\n",
    "    \n",
    "        \n",
    "    df = pd.DataFrame(df_dict)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def read_message(directory):\n",
    "    with open(directory, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        message = f.read()\n",
    "    \n",
    "    return message\n",
    "\n",
    "\n",
    "def email_parser(input_file):\n",
    "    with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        parsed_email = email.message_from_string(f.read())\n",
    "    \n",
    "    return parsed_email\n",
    "\n",
    "\n",
    "def get_email_id(input_file):\n",
    "    email_dir = '/'.join(str(input_file).split('/')[5:])\n",
    "    \n",
    "    return email_dir\n",
    "\n",
    "\n",
    "def preprocess_recipients(recipient):\n",
    "    if recipient is not None:\n",
    "        users = re.sub(r'\\s+', '', recipient).split(',')\n",
    "        if len(users) > 1:\n",
    "            return users\n",
    "        else:\n",
    "            return users[0]\n",
    "\n",
    "\n",
    "def get_timestamp(input_string):\n",
    "    parsed_dt = parser.parse(input_string)\n",
    "    \n",
    "    return parsed_dt.timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1be749a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.8 s, sys: 719 ms, total: 34.5 s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Check whether we know where to find FreeLing data files\n",
    "if \"FREELINGDIR\" not in os.environ:\n",
    "    if sys.platform == \"win32\" or sys.platform == \"win64\":\n",
    "        os.environ[\"FREELINGDIR\"] = \"C:\\\\Program Files\"\n",
    "    else:\n",
    "        os.environ[\"FREELINGDIR\"] = \"/usr/local\"\n",
    "    print(\n",
    "        \"FREELINGDIR environment variable not defined, trying \",\n",
    "        os.environ[\"FREELINGDIR\"],\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "if not os.path.exists(os.environ[\"FREELINGDIR\"] + \"/share/freeling\"):\n",
    "    print(\n",
    "        \"Folder\",\n",
    "        os.environ[\"FREELINGDIR\"] + \"/share/freeling\",\n",
    "        \"not found.\\n\" +\n",
    "        \"Please set FREELINGDIR environment variable to FreeLing installation directory\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    sys.exit(1)\n",
    "\n",
    "# Location of FreeLing configuration files.\n",
    "DATA = os.environ[\"FREELINGDIR\"] + \"/share/freeling/\"\n",
    "\n",
    "# Init locales\n",
    "pyfreeling.util_init_locale(\"default\")\n",
    "\n",
    "# create language detector. Used just to show it. Results are printed\n",
    "# but ignored (after, it is assumed language is LANG)\n",
    "# la = pyfreeling.lang_ident(DATA + \"common/lang_ident/ident-few.dat\")\n",
    "\n",
    "# create options set for maco analyzer.\n",
    "# Default values are Ok, except for data files.\n",
    "LANG = 'en'\n",
    "op = pyfreeling.maco_options(LANG)\n",
    "op.set_data_files(\n",
    "    \"\",\n",
    "    DATA + \"common/punct.dat\",\n",
    "    DATA + LANG + \"/dicc.src\",\n",
    "    DATA + LANG + \"/afixos.dat\",\n",
    "    \"\",\n",
    "    DATA + LANG + \"/locucions.dat\",\n",
    "    DATA + LANG + \"/np.dat\",\n",
    "    DATA + LANG + \"/quantities.dat\",\n",
    "    DATA + LANG + \"/probabilitats.dat\",\n",
    ")\n",
    "\n",
    "# create analyzers\n",
    "tk = pyfreeling.tokenizer(DATA + LANG + \"/tokenizer.dat\")\n",
    "sp = pyfreeling.splitter(DATA + LANG + \"/splitter.dat\")\n",
    "sid = sp.open_session()\n",
    "mf = pyfreeling.maco(op)\n",
    "\n",
    "# activate morpho modules to be used in next call\n",
    "mf.set_active_options(\n",
    "    False,  # UserMap\n",
    "    True,  # NumbersDetection\n",
    "    True,  # PunctuationDetection\n",
    "    True,  # DatesDetection\n",
    "    True,  # DictionarySearch\n",
    "    True,  # AffixAnalysis\n",
    "    False,  # CompoundAnalysis\n",
    "    True,  # RetokContractions\n",
    "    True,  # MultiwordsDetection\n",
    "    True,  # NERecognition\n",
    "    True,  # QuantitiesDetection\n",
    "    True  # ProbabilityAssignment\n",
    ")\n",
    "# default: all created submodules are used\n",
    "\n",
    "# create tagger, sense anotator, and parsers\n",
    "tg = pyfreeling.hmm_tagger(DATA + LANG + \"/tagger.dat\", True, 2)\n",
    "sen = pyfreeling.senses(DATA + LANG + \"/senses.dat\")\n",
    "dep = pyfreeling.dep_lstm(\n",
    "    DATA + LANG + \"/dep_lstm/params-en.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a5614e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 304 ms, sys: 199 ms, total: 503 ms\n",
      "Wall time: 491 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if Path('enron_mails.p').is_file():\n",
    "    df = pd.read_pickle('enron_mails.p')\n",
    "else:\n",
    "    emails_path = Path(Path.cwd().parent, 'maildir')\n",
    "    emails_list = emails_path.rglob('*.')\n",
    "    # df = process_emails(emails_list, To=True, From=True, Date=True)\n",
    "    df = process_emails(emails_list)\n",
    "    df.to_pickle('enron_mails.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee40a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forms(text):\n",
    "    tokens = []\n",
    "    lw = tk.tokenize(text)\n",
    "    ls = sp.split(sid, lw, True)\n",
    "    for s in ls:\n",
    "        for w in s:\n",
    "            tokens.append(w.get_form())\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_lemmas(text):\n",
    "    lemmas = []\n",
    "    lw = tk.tokenize(text)\n",
    "    ls = sp.split(sid, lw, True)\n",
    "    ls = mf.analyze(ls)\n",
    "    for s in ls:\n",
    "        for w in s:\n",
    "            lemmas.append(w.get_lemma())\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def get_pos(text):\n",
    "    pos = []\n",
    "    lw = tk.tokenize(text)\n",
    "    ls = sp.split(sid, lw, True)\n",
    "    ls = tg.analyze(ls)\n",
    "    for s in ls:\n",
    "        for w in s:\n",
    "            pos.append(w.get_tag())\n",
    "    \n",
    "    return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8321e630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPLITTER: Ridiculously long sentence between markers at token '%0D%0A' at input offset 7850.\n",
      "SPLITTER: Ridiculously long sentence between markers at token 'of' at input offset 5439.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '/' at input offset 30033.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '/' at input offset 30810.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '/' at input offset 30595.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '/' at input offset 32344.\n",
      "SPLITTER: Ridiculously long sentence between markers at token 'on' at input offset 11217.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '25' at input offset 5801.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '25' at input offset 5801.\n",
      "SPLITTER: Ridiculously long sentence between markers at token ',' at input offset 10885.\n",
      "SPLITTER: Ridiculously long sentence between markers at token ',' at input offset 6483.\n",
      "SPLITTER: Ridiculously long sentence between markers at token 'on' at input offset 11217.\n",
      "SPLITTER: Ridiculously long sentence between markers at token 'A' at input offset 3813.\n",
      "SPLITTER: Ridiculously long sentence between markers at token 'on' at input offset 11217.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '25' at input offset 5801.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '/' at input offset 161333.\n",
      "SPLITTER: Ridiculously long sentence between markers at token ':49' at input offset 1015040.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['forms'] = df['text'].apply(get_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46433765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPLITTER: Ridiculously long sentence between markers at token '%0D%0A' at input offset 7850.\n",
      "SPLITTER: Ridiculously long sentence between markers at token 'of' at input offset 5439.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '/' at input offset 30033.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '/' at input offset 30810.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '/' at input offset 30595.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '/' at input offset 32344.\n",
      "SPLITTER: Ridiculously long sentence between markers at token 'on' at input offset 11217.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '25' at input offset 5801.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '25' at input offset 5801.\n",
      "SPLITTER: Ridiculously long sentence between markers at token ',' at input offset 10885.\n",
      "SPLITTER: Ridiculously long sentence between markers at token ',' at input offset 6483.\n",
      "SPLITTER: Ridiculously long sentence between markers at token 'on' at input offset 11217.\n",
      "SPLITTER: Ridiculously long sentence between markers at token 'A' at input offset 3813.\n",
      "SPLITTER: Ridiculously long sentence between markers at token 'on' at input offset 11217.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '25' at input offset 5801.\n",
      "SPLITTER: Ridiculously long sentence between markers at token '/' at input offset 161333.\n",
      "SPLITTER: Ridiculously long sentence between markers at token ':49' at input offset 1015040.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['lemmas'] = df['text'].apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('emails_fl.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = df.loc[3,'text']\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_forms(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lemmas(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec62b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae46c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53706045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243b682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c36a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9516be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in ls:\n",
    "    for w in s:\n",
    "        print(w.get_form())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8961bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def obtain_lemmas(self, text):\n",
    "    results = {}\n",
    "    # for lin in io.StringIO(text.get_payload()):\n",
    "    for lin in io.StringIO(text):\n",
    "        lw = self.tk.tokenize(lin.strip())\n",
    "        ls = self.sp.split(self.sid, lw, False)\n",
    "        ls = self.mf.analyze(ls)\n",
    "        for s in ls:\n",
    "            ws = s.get_words()\n",
    "            for w in ws:\n",
    "                key = f'{w.get_form()}_Lemma_{w.get_lemma()}'\n",
    "                add_to_dict(key, results)\n",
    "    return results\n",
    "\n",
    "def obtain_pos(self, text):\n",
    "    results = {}\n",
    "    # for lin in io.StringIO(text.get_payload()):\n",
    "    for lin in io.StringIO(text):\n",
    "        lw = self.tk.tokenize(lin.strip())\n",
    "        ls = self.sp.split(self.sid, lw, False)\n",
    "        ls = self.tg.analyze(ls)\n",
    "        for s in ls:\n",
    "            ws = s.get_words()\n",
    "            for w in ws:\n",
    "                key = f'{w.get_form()}_PoS_{w.get_tag()}'\n",
    "                add_to_dict(key, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cda20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train, test_size=0.4, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c94b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515916db",
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bd5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miri] *",
   "language": "python",
   "name": "conda-env-miri-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
