{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "080d4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "\n",
    "import pyfreeling\n",
    "\n",
    "import os\n",
    "from email.parser import Parser\n",
    "import email.utils\n",
    "\n",
    "import time\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb49da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"../maildir/lay-k\"  # Change root dir to affect how many mails are touched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24cfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messageIDtoSubject(mail_dict, messageID):\n",
    "    return mail_dict[messageID][\"subject\"].replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def raw_parse(inputfile, email_list):\n",
    "    with open(inputfile, \"r\",encoding=\"utf-8\",errors=\"ignore\") as f:\n",
    "        data = f.read()\n",
    "    parsedEmail = Parser().parsestr(data)\n",
    "    timestamp = time.mktime(email.utils.parsedate(parsedEmail[\"date\"]))\n",
    "    email_list.append((timestamp, parsedEmail))\n",
    "\n",
    "\n",
    "def obtain_raw_threads(mail_dict, email_list):\n",
    "    subject_dict = {}\n",
    "    rootMailSet = set()\n",
    "    auxiliarRootStructure = {}\n",
    "\n",
    "    for mail in email_list:\n",
    "        actualEmail = mail[1]\n",
    "        true_subject = actualEmail['subject']\n",
    "        true_subject = true_subject.replace(\" \", \"\")\n",
    "        mail_dict[actualEmail[\"message-id\"]] = actualEmail\n",
    "        if \"Re:\" != true_subject[0:3]:\n",
    "            auxiliarRootStructure[true_subject] = actualEmail[\"message-id\"]\n",
    "            if true_subject not in subject_dict:\n",
    "                subject_dict[true_subject] = []\n",
    "        else:\n",
    "            new_subject = true_subject.replace(\"Re:\", \"\")\n",
    "            if new_subject in subject_dict:\n",
    "                subject_dict[new_subject].append(actualEmail[\"message-id\"])\n",
    "                rootMailSet.add(auxiliarRootStructure[new_subject])\n",
    "            subject_dict[true_subject] = []\n",
    "    threads = {}\n",
    "    for mail in rootMailSet:\n",
    "        subject = messageIDtoSubject(mail_dict, mail)\n",
    "        childThread = subject_dict[subject]\n",
    "        threads[mail] = childThread\n",
    "    return threads\n",
    "\n",
    "\n",
    "def preprocess_recipients(recipient):\n",
    "    if recipient is not None:\n",
    "        users = re.sub(r'\\s+', '', recipient).split(',')\n",
    "        if len(users) > 1:\n",
    "            return users\n",
    "        else:\n",
    "            return users[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def obtain_base_features(mail):\n",
    "    email_dict={}\n",
    "    email_dict[mail['message-id']] = {\n",
    "            'from': mail['from'],\n",
    "            # 'to': preprocess_recipients(mail['to']),\n",
    "            'date': mail['date'],\n",
    "        }\n",
    "    return email_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4ec5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'en'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7e159d9",
   "metadata": {},
   "source": [
    "if \"FREELINGDIR\" not in os.environ:\n",
    "    if sys.platform == \"win32\" or sys.platform == \"win64\":\n",
    "        os.environ[\"FREELINGDIR\"] = \"C:\\\\Program Files\"\n",
    "    else:\n",
    "        os.environ[\"FREELINGDIR\"] = \"/usr/local\"\n",
    "    print(\n",
    "        \"FREELINGDIR environment variable not defined, trying \",\n",
    "        os.environ[\"FREELINGDIR\"],\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "if not os.path.exists(os.environ[\"FREELINGDIR\"] + \"/share/freeling\"):\n",
    "    print(\n",
    "        \"Folder\",\n",
    "        os.environ[\"FREELINGDIR\"] + \"/share/freeling\",\n",
    "        \"not found.\\n\" +\n",
    "        \"Please set FREELINGDIR environment variable to FreeLing installation directory\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    sys.exit(1)\n",
    "\n",
    "# Location of FreeLing configuration files.\n",
    "DATA = os.environ[\"FREELINGDIR\"] + \"/share/freeling/\"\n",
    "\n",
    "# Init locales\n",
    "pyfreeling.util_init_locale(\"default\")\n",
    "\n",
    "# create language detector. Used just to show it. Results are printed\n",
    "# but ignored (after, it is assumed language is LANG)\n",
    "# la = pyfreeling.lang_ident(DATA + \"common/lang_ident/ident-few.dat\")\n",
    "\n",
    "# create options set for maco analyzer.\n",
    "# Default values are Ok, except for data files.\n",
    "LANG = lang\n",
    "op = pyfreeling.maco_options(LANG)\n",
    "op.set_data_files(\n",
    "    \"\",\n",
    "    DATA + \"common/punct.dat\",\n",
    "    DATA + LANG + \"/dicc.src\",\n",
    "    DATA + LANG + \"/afixos.dat\",\n",
    "    \"\",\n",
    "    DATA + LANG + \"/locucions.dat\",\n",
    "    DATA + LANG + \"/np.dat\",\n",
    "    DATA + LANG + \"/quantities.dat\",\n",
    "    DATA + LANG + \"/probabilitats.dat\",\n",
    ")\n",
    "\n",
    "# create analyzers\n",
    "tk = pyfreeling.tokenizer(DATA + LANG + \"/tokenizer.dat\")\n",
    "sp = pyfreeling.splitter(DATA + LANG + \"/splitter.dat\")\n",
    "sid = sp.open_session()\n",
    "mf = pyfreeling.maco(op)\n",
    "\n",
    "# activate morpho modules to be used in next call\n",
    "mf.set_active_options(\n",
    "    False, # UserMap\n",
    "    True,  # NumbersDetection\n",
    "    True,  # PunctuationDetection\n",
    "    True,  # DatesDetection\n",
    "    True,  # DictionarySearch\n",
    "    True,  # AffixAnalysis\n",
    "    False, # CompoundAnalysis\n",
    "    True,  # RetokContractions\n",
    "    True,  # MultiwordsDetection\n",
    "    True,  # NERecognition\n",
    "    True,  # QuantitiesDetection\n",
    "    True,  # ProbabilityAssignment\n",
    ")\n",
    "# default: all created submodules are used\n",
    "\n",
    "# create tagger, sense anotator, and parsers\n",
    "tg = pyfreeling.hmm_tagger(DATA + LANG + \"/tagger.dat\", True, 2)\n",
    "sen = pyfreeling.senses(DATA + LANG + \"/senses.dat\")\n",
    "dep = pyfreeling.dep_lstm(\n",
    "    DATA + LANG + \"/dep_lstm/params-en.dat\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "889f6f8a",
   "metadata": {},
   "source": [
    "def obtain_tokens(text):\n",
    "    results = {}\n",
    "    for lin in io.StringIO(actualEmail.get_payload()):\n",
    "        if lin.strip():\n",
    "            lw = tk.tokenize(lin)\n",
    "            ls = sp.split(sid, lw, False)\n",
    "            if len(ls) > 0:\n",
    "                ws = ls[0].get_words()\n",
    "                for w in ws:\n",
    "                    key = w.get_form()\n",
    "                    add_to_dict(key, results)\n",
    "    return results\n",
    "\n",
    "def obtain_lemmas(text):\n",
    "    results = {}\n",
    "    for lin in io.StringIO(actualEmail.get_payload()):\n",
    "        if lin.strip():\n",
    "            lw = tk.tokenize(lin)\n",
    "            ls = sp.split(sid, lw, False)\n",
    "            ls = mf.analyze(ls)\n",
    "            if len(ls) > 0:\n",
    "                ws = ls[0].get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}_{w.get_lemma()}'\n",
    "                    add_to_dict(key, results)\n",
    "    return results\n",
    "\n",
    "def obtain_pos(text):\n",
    "    results = {}\n",
    "    for lin in io.StringIO(actualEmail.get_payload()):\n",
    "        if lin.strip():\n",
    "            lw = tk.tokenize(lin)\n",
    "            ls = sp.split(sid, lw, False)\n",
    "            ls = mf.analyze(ls)\n",
    "            ls = tg.analyze(ls)\n",
    "            if len(ls) > 0:\n",
    "                ws = ls[0].get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}_{w.get_tag()}'\n",
    "                    add_to_dict(key, results)\n",
    "    return results\n",
    "\n",
    "def process(text, token=True, lemma=False, pos=False):\n",
    "    features = {}\n",
    "    if token:\n",
    "        features['tokens'] = obtain_tokens(text)\n",
    "    if lemma:\n",
    "        features['lemmas'] = obtain_lemmas(text)\n",
    "    if pos:\n",
    "        features['PoS'] = obtain_pos(text)\n",
    "    return features\n",
    "\n",
    "def add_to_dict(key, feature_Dictionary):\n",
    "    if key in feature_Dictionary:\n",
    "        feature_Dictionary[key] += 1\n",
    "    else:\n",
    "        feature_Dictionary[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33832e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dict(key, feature_Dictionary):\n",
    "    if key in feature_Dictionary:\n",
    "        feature_Dictionary[key] += 1\n",
    "    else:\n",
    "        feature_Dictionary[key] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0e0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_dict = {}\n",
    "email_list = []\n",
    "\n",
    "for directory, subdirectory, filenames in os.walk(basedir):\n",
    "    for filename in filenames:\n",
    "        raw_parse(os.path.join(directory, filename), email_list)\n",
    "email_list.sort(key=lambda x: x[0])\n",
    "\n",
    "# mail_dict = obtain_base_features(email_list)\n",
    "\n",
    "pureThreads = obtain_raw_threads(mail_dict, email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327d3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mailsWithFeatures = {}\n",
    "for mail in email_list[:2]:\n",
    "    actualEmail = mail[1]\n",
    "    mailsWithFeatures[mail] = obtain_base_features(actualEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d12ed692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreelingAnalyzer(object):\n",
    "    def __init__(self, folder, lang):\n",
    "        self.folder = folder\n",
    "        self.lang = lang\n",
    "        self.tk = None\n",
    "        self.sp = None\n",
    "        self.sid = None\n",
    "        self.mf = None\n",
    "        self.tg = None\n",
    "        self.sen = None\n",
    "        # self.parser = None\n",
    "        self.dep = None\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        # Check whether we know where to find FreeLing data files\n",
    "        if \"FREELINGDIR\" not in os.environ:\n",
    "            if sys.platform == \"win32\" or sys.platform == \"win64\":\n",
    "                os.environ[\"FREELINGDIR\"] = \"C:\\\\Program Files\"\n",
    "            else:\n",
    "                os.environ[\"FREELINGDIR\"] = \"/usr/local\"\n",
    "            print(\n",
    "                \"FREELINGDIR environment variable not defined, trying \",\n",
    "                os.environ[\"FREELINGDIR\"],\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "\n",
    "        if not os.path.exists(os.environ[\"FREELINGDIR\"] + \"/share/freeling\"):\n",
    "            print(\n",
    "                \"Folder\",\n",
    "                os.environ[\"FREELINGDIR\"] + \"/share/freeling\",\n",
    "                \"not found.\\n\" +\n",
    "                \"Please set FREELINGDIR environment variable to FreeLing installation directory\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Location of FreeLing configuration files.\n",
    "        DATA = os.environ[\"FREELINGDIR\"] + \"/share/freeling/\"\n",
    "\n",
    "        # Init locales\n",
    "        pyfreeling.util_init_locale(\"default\")\n",
    "\n",
    "        # create language detector. Used just to show it. Results are printed\n",
    "        # but ignored (after, it is assumed language is LANG)\n",
    "        # la = pyfreeling.lang_ident(DATA + \"common/lang_ident/ident-few.dat\")\n",
    "\n",
    "        # create options set for maco analyzer.\n",
    "        # Default values are Ok, except for data files.\n",
    "        LANG = self.lang\n",
    "        op = pyfreeling.maco_options(LANG)\n",
    "        op.set_data_files(\n",
    "            \"\",\n",
    "            DATA + \"common/punct.dat\",\n",
    "            DATA + LANG + \"/dicc.src\",\n",
    "            DATA + LANG + \"/afixos.dat\",\n",
    "            \"\",\n",
    "            DATA + LANG + \"/locucions.dat\",\n",
    "            DATA + LANG + \"/np.dat\",\n",
    "            DATA + LANG + \"/quantities.dat\",\n",
    "            DATA + LANG + \"/probabilitats.dat\",\n",
    "        )\n",
    "\n",
    "        # create analyzers\n",
    "        self.tk = pyfreeling.tokenizer(DATA + LANG + \"/tokenizer.dat\")\n",
    "        self.sp = pyfreeling.splitter(DATA + LANG + \"/splitter.dat\")\n",
    "        self.sid = self.sp.open_session()\n",
    "        self.mf = pyfreeling.maco(op)\n",
    "\n",
    "        # activate morpho modules to be used in next call\n",
    "        self.mf.set_active_options(\n",
    "            False, # UserMap\n",
    "            True,  # NumbersDetection\n",
    "            True,  # PunctuationDetection\n",
    "            True,  # DatesDetection\n",
    "            True,  # DictionarySearch\n",
    "            True,  # AffixAnalysis\n",
    "            False, # CompoundAnalysis\n",
    "            True,  # RetokContractions\n",
    "            True,  # MultiwordsDetection\n",
    "            True,  # NERecognition\n",
    "            True,  # QuantitiesDetection\n",
    "            True   # ProbabilityAssignment\n",
    "        )\n",
    "        # default: all created submodules are used\n",
    "\n",
    "        # create tagger, sense anotator, and parsers\n",
    "        self.tg = pyfreeling.hmm_tagger(DATA + LANG + \"/tagger.dat\", True, 2)\n",
    "        self.sen = pyfreeling.senses(DATA + LANG + \"/senses.dat\")\n",
    "        self.dep = pyfreeling.dep_lstm(\n",
    "            DATA + LANG + \"/dep_lstm/params-en.dat\")\n",
    "\n",
    "    def obtain_tokens_alt(self, text, feature_dict):\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            lw = self.tk.tokenize(lin.strip())\n",
    "            ls = self.sp.split(self.sid, lw, False)\n",
    "            #if lin.strip():\n",
    "             #   lw = self.tk.tokenize(lin)\n",
    "             #   ls = self.sp.split(self.sid, lw, False)\n",
    "            #if len(ls) > 0:\n",
    "            for s in ls:\n",
    "                ws = s.get_words()\n",
    "                for w in ws:\n",
    "                    key = w.get_form()\n",
    "                    add_to_dict(key, feature_dict)\n",
    "            #return results\n",
    "    \n",
    "    def process(self, text, token=True, lemma=False, pos=False):\n",
    "        results = {}\n",
    "        ls = []\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            lw = self.tk.tokenize(lin.strip())\n",
    "            ls.append(self.sp.split(self.sid, lw, False))\n",
    "        \n",
    "        if token:\n",
    "            self.obtain_tokens(ls, results)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    #def obtain_tokens(self, ls, feature_dict):\n",
    "    def obtain_tokens(self, text, False):\n",
    "        results = {}\n",
    "        lw = []\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            lw.append(self.tk.tokenize(lin.strip()))\n",
    "        \n",
    "        ls = []\n",
    "        for w in lw:\n",
    "            ls.append(self.sp.split(self.sid, w))\n",
    "            \n",
    "        ws = []\n",
    "        for s in ls:\n",
    "            if len(s) > 0:\n",
    "                ws.append(s[0].get_words())\n",
    "\n",
    "        for w in ws:\n",
    "            for t in w:\n",
    "                key = t.get_form()\n",
    "                add_to_dict(key, results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def obtain_lemmas(self, ls, feature_dict):\n",
    "        ls = self.mf.analyze(ls)\n",
    "        ws = []\n",
    "        for s in ls:\n",
    "            if len(s) > 0:\n",
    "                ws.append(s[0].get_words())\n",
    "\n",
    "        for w in ws:\n",
    "            for t in w:\n",
    "                key = f'{w.get_form()}_Lemma_{w.get_lemma()}'\n",
    "                add_to_dict(key, feature_dict)\n",
    "\n",
    "    def obtain_pos(self, ls, feature_dict):\n",
    "        ls = self.tg.analyze(ls)\n",
    "        ws = []\n",
    "        for s in ls:\n",
    "            if len(s) > 0:\n",
    "                ws.append(s[0].get_words())\n",
    "\n",
    "        for w in ws:\n",
    "            for t in w:\n",
    "                key = f'{w.get_form()}_PoS_{w.get_tag()}'\n",
    "                add_to_dict(key, feature_dict)\n",
    "    \n",
    "    def obtain_lemmas_alt(self, text):\n",
    "        results = {}\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            lw = self.tk.tokenize(lin.strip())\n",
    "            ls = self.sp.split(self.sid, lw, False)\n",
    "            ls = self.mf.analyze(ls)\n",
    "            for s in ls:\n",
    "                ws = s.get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}_Lemma_{w.get_lemma()}'\n",
    "                    add_to_dict(key, results)\n",
    "        return results\n",
    "    \n",
    "    def obtain_pos_alt(self, text):\n",
    "        results = {}\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            lw = self.tk.tokenize(lin.strip())\n",
    "            ls = self.sp.split(self.sid, lw, False)\n",
    "            ls = self.tg.analyze(ls)\n",
    "            for s in ls:\n",
    "                ws = s.get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}_PoS_{w.get_tag()}'\n",
    "                    add_to_dict(key, results)\n",
    "        return results\n",
    "    \n",
    "    def close(self):\n",
    "        self.sp.close_session(self.sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c2f30d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 s, sys: 512 ms, total: 30.3 s\n",
      "Wall time: 30 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "anal = FreelingAnalyzer(basedir, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce892fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling All Alumni!\n",
      "\n",
      "University of Houston, University of Houston-Clear Lake, University of\n",
      "Houston-Downtown, University of Houston-Victoria\n",
      "You are cordially invited to a reception\n",
      "hosted by\n",
      "Ken Lay and Chancellor Arthur K. Smith\n",
      "\n",
      "Learn more about the Enron/U of H partnership!\n",
      "\n",
      "Thursday, October 7, 1999\n",
      "5:30 - 7:00 p.m.\n",
      "50th Floor, Enron Building\n",
      "\n",
      "R.S.V.P. to Patti Walsh via Lotus Notes by October 4, 1999\n"
     ]
    }
   ],
   "source": [
    "for lin in io.StringIO(actualEmail.get_payload()):\n",
    "    print(lin.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb79644",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong number or type of arguments for overloaded function 'splitter_split'.\n  Possible C/C++ prototypes are:\n    freeling::splitter::split(freeling::splitter::session_id,std::list< freeling::word,std::allocator< freeling::word > > const &,bool,std::list< freeling::sentence,std::allocator< freeling::sentence > > &) const\n    freeling::splitter::split(freeling::splitter::session_id,std::list< freeling::word,std::allocator< freeling::word > > const &,bool) const\n    freeling::splitter::split(std::list< freeling::word,std::allocator< freeling::word > > const &,std::list< freeling::sentence,std::allocator< freeling::sentence > > &) const\n    freeling::splitter::split(std::list< freeling::word,std::allocator< freeling::word > > const &) const\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#results = {}\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#anal.obtain_tokens(actualEmail, results)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43manal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactualEmail\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mFreelingAnalyzer.obtain_tokens\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    127\u001b[0m ls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m lw:\n\u001b[0;32m--> 129\u001b[0m     ls\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    131\u001b[0m ws \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m ls:\n",
      "File \u001b[0;32m~/Programs/freeling/share/freeling/APIs/python3/pyfreeling.py:4782\u001b[0m, in \u001b[0;36msplitter.split\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   4781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd::list< freeling::sentence,std::allocator< freeling::sentence > >\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pyfreeling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrong number or type of arguments for overloaded function 'splitter_split'.\n  Possible C/C++ prototypes are:\n    freeling::splitter::split(freeling::splitter::session_id,std::list< freeling::word,std::allocator< freeling::word > > const &,bool,std::list< freeling::sentence,std::allocator< freeling::sentence > > &) const\n    freeling::splitter::split(freeling::splitter::session_id,std::list< freeling::word,std::allocator< freeling::word > > const &,bool) const\n    freeling::splitter::split(std::list< freeling::word,std::allocator< freeling::word > > const &,std::list< freeling::sentence,std::allocator< freeling::sentence > > &) const\n    freeling::splitter::split(std::list< freeling::word,std::allocator< freeling::word > > const &) const\n"
     ]
    }
   ],
   "source": [
    "#results = {}\n",
    "#anal.obtain_tokens(actualEmail, results)\n",
    "anal.obtain_tokens(actualEmail)\n",
    "#anal.process(actualEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de59549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = True\n",
    "mailsWithFeatures = {}\n",
    "for mail in email_list[:2]:\n",
    "    actualEmail = mail[1]\n",
    "    mailsWithFeatures[mail] = obtain_base_features(actualEmail)\n",
    "    if token:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['tokens'] = anal.obtain_tokens(actualEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mailsWithFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = True\n",
    "lemma = True\n",
    "mailsWithFeatures = {}\n",
    "for mail in email_list[:2]:\n",
    "    actualEmail = mail[1]\n",
    "    mailsWithFeatures[mail] = obtain_base_features(actualEmail)\n",
    "    if token:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['tokens'] = anal.obtain_tokens(actualEmail)\n",
    "    if lemma:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['lemmas'] = anal.obtain_lemmas(actualEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b867617",
   "metadata": {},
   "outputs": [],
   "source": [
    "mailsWithFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f818492",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = False\n",
    "lemma = False\n",
    "pos = True\n",
    "mailsWithFeatures = {}\n",
    "for mail in email_list[:2]:\n",
    "    actualEmail = mail[1]\n",
    "    mailsWithFeatures[mail] = obtain_base_features(actualEmail)\n",
    "    if token:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['tokens'] = anal.obtain_tokens(actualEmail)\n",
    "    if lemma:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['lemmas'] = anal.obtain_lemmas(actualEmail)\n",
    "    if pos:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['pos'] = anal.obtain_pos(actualEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79550e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "mailsWithFeatures"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7848ad99",
   "metadata": {},
   "source": [
    "sp.close_session(sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d166645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miri] *",
   "language": "python",
   "name": "conda-env-miri-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
