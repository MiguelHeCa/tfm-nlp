{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "\n",
    "import pyfreeling\n",
    "\n",
    "import os\n",
    "from email.parser import Parser\n",
    "import email.utils\n",
    "\n",
    "import time\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb49da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"../maildir/lay-k\"  # Change root dir to affect how many mails are touched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messageIDtoSubject(mail_dict, messageID):\n",
    "    return mail_dict[messageID][\"subject\"].replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def raw_parse(inputfile, email_list):\n",
    "    with open(inputfile, \"r\",encoding=\"utf-8\",errors=\"ignore\") as f:\n",
    "        data = f.read()\n",
    "    parsedEmail = Parser().parsestr(data)\n",
    "    timestamp = time.mktime(email.utils.parsedate(parsedEmail[\"date\"]))\n",
    "    email_list.append((timestamp, parsedEmail))\n",
    "\n",
    "\n",
    "def obtain_raw_threads(mail_dict, email_list):\n",
    "    subject_dict = {}\n",
    "    rootMailSet = set()\n",
    "    auxiliarRootStructure = {}\n",
    "\n",
    "    for mail in email_list:\n",
    "        actualEmail = mail[1]\n",
    "        true_subject = actualEmail['subject']\n",
    "        true_subject = true_subject.replace(\" \", \"\")\n",
    "        mail_dict[actualEmail[\"message-id\"]] = actualEmail\n",
    "        if \"Re:\" != true_subject[0:3]:\n",
    "            auxiliarRootStructure[true_subject] = actualEmail[\"message-id\"]\n",
    "            if true_subject not in subject_dict:\n",
    "                subject_dict[true_subject] = []\n",
    "        else:\n",
    "            new_subject = true_subject.replace(\"Re:\", \"\")\n",
    "            if new_subject in subject_dict:\n",
    "                subject_dict[new_subject].append(actualEmail[\"message-id\"])\n",
    "                rootMailSet.add(auxiliarRootStructure[new_subject])\n",
    "            subject_dict[true_subject] = []\n",
    "    threads = {}\n",
    "    for mail in rootMailSet:\n",
    "        subject = messageIDtoSubject(mail_dict, mail)\n",
    "        childThread = subject_dict[subject]\n",
    "        threads[mail] = childThread\n",
    "    return threads\n",
    "\n",
    "\n",
    "def preprocess_recipients(recipient):\n",
    "    if recipient is not None:\n",
    "        users = re.sub(r'\\s+', '', recipient).split(',')\n",
    "        if len(users) > 1:\n",
    "            return users\n",
    "        else:\n",
    "            return users[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def obtain_base_features(mail):\n",
    "    email_dict={}\n",
    "    email_dict[mail['message-id']] = {\n",
    "            'from': mail['from'],\n",
    "            # 'to': preprocess_recipients(mail['to']),\n",
    "            'date': mail['date'],\n",
    "        }\n",
    "    return email_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ec5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'en'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7e159d9",
   "metadata": {},
   "source": [
    "if \"FREELINGDIR\" not in os.environ:\n",
    "    if sys.platform == \"win32\" or sys.platform == \"win64\":\n",
    "        os.environ[\"FREELINGDIR\"] = \"C:\\\\Program Files\"\n",
    "    else:\n",
    "        os.environ[\"FREELINGDIR\"] = \"/usr/local\"\n",
    "    print(\n",
    "        \"FREELINGDIR environment variable not defined, trying \",\n",
    "        os.environ[\"FREELINGDIR\"],\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "if not os.path.exists(os.environ[\"FREELINGDIR\"] + \"/share/freeling\"):\n",
    "    print(\n",
    "        \"Folder\",\n",
    "        os.environ[\"FREELINGDIR\"] + \"/share/freeling\",\n",
    "        \"not found.\\n\" +\n",
    "        \"Please set FREELINGDIR environment variable to FreeLing installation directory\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    sys.exit(1)\n",
    "\n",
    "# Location of FreeLing configuration files.\n",
    "DATA = os.environ[\"FREELINGDIR\"] + \"/share/freeling/\"\n",
    "\n",
    "# Init locales\n",
    "pyfreeling.util_init_locale(\"default\")\n",
    "\n",
    "# create language detector. Used just to show it. Results are printed\n",
    "# but ignored (after, it is assumed language is LANG)\n",
    "# la = pyfreeling.lang_ident(DATA + \"common/lang_ident/ident-few.dat\")\n",
    "\n",
    "# create options set for maco analyzer.\n",
    "# Default values are Ok, except for data files.\n",
    "LANG = lang\n",
    "op = pyfreeling.maco_options(LANG)\n",
    "op.set_data_files(\n",
    "    \"\",\n",
    "    DATA + \"common/punct.dat\",\n",
    "    DATA + LANG + \"/dicc.src\",\n",
    "    DATA + LANG + \"/afixos.dat\",\n",
    "    \"\",\n",
    "    DATA + LANG + \"/locucions.dat\",\n",
    "    DATA + LANG + \"/np.dat\",\n",
    "    DATA + LANG + \"/quantities.dat\",\n",
    "    DATA + LANG + \"/probabilitats.dat\",\n",
    ")\n",
    "\n",
    "# create analyzers\n",
    "tk = pyfreeling.tokenizer(DATA + LANG + \"/tokenizer.dat\")\n",
    "sp = pyfreeling.splitter(DATA + LANG + \"/splitter.dat\")\n",
    "sid = sp.open_session()\n",
    "mf = pyfreeling.maco(op)\n",
    "\n",
    "# activate morpho modules to be used in next call\n",
    "mf.set_active_options(\n",
    "    False, # UserMap\n",
    "    True,  # NumbersDetection\n",
    "    True,  # PunctuationDetection\n",
    "    True,  # DatesDetection\n",
    "    True,  # DictionarySearch\n",
    "    True,  # AffixAnalysis\n",
    "    False, # CompoundAnalysis\n",
    "    True,  # RetokContractions\n",
    "    True,  # MultiwordsDetection\n",
    "    True,  # NERecognition\n",
    "    True,  # QuantitiesDetection\n",
    "    True,  # ProbabilityAssignment\n",
    ")\n",
    "# default: all created submodules are used\n",
    "\n",
    "# create tagger, sense anotator, and parsers\n",
    "tg = pyfreeling.hmm_tagger(DATA + LANG + \"/tagger.dat\", True, 2)\n",
    "sen = pyfreeling.senses(DATA + LANG + \"/senses.dat\")\n",
    "dep = pyfreeling.dep_lstm(\n",
    "    DATA + LANG + \"/dep_lstm/params-en.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b5ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02375e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "889f6f8a",
   "metadata": {},
   "source": [
    "def obtain_tokens(text):\n",
    "    results = {}\n",
    "    for lin in io.StringIO(actualEmail.get_payload()):\n",
    "        if lin.strip():\n",
    "            lw = tk.tokenize(lin)\n",
    "            ls = sp.split(sid, lw, False)\n",
    "            if len(ls) > 0:\n",
    "                ws = ls[0].get_words()\n",
    "                for w in ws:\n",
    "                    key = w.get_form()\n",
    "                    add_to_dict(key, results)\n",
    "    return results\n",
    "\n",
    "def obtain_lemmas(text):\n",
    "    results = {}\n",
    "    for lin in io.StringIO(actualEmail.get_payload()):\n",
    "        if lin.strip():\n",
    "            lw = tk.tokenize(lin)\n",
    "            ls = sp.split(sid, lw, False)\n",
    "            ls = mf.analyze(ls)\n",
    "            if len(ls) > 0:\n",
    "                ws = ls[0].get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}_{w.get_lemma()}'\n",
    "                    add_to_dict(key, results)\n",
    "    return results\n",
    "\n",
    "def obtain_pos(text):\n",
    "    results = {}\n",
    "    for lin in io.StringIO(actualEmail.get_payload()):\n",
    "        if lin.strip():\n",
    "            lw = tk.tokenize(lin)\n",
    "            ls = sp.split(sid, lw, False)\n",
    "            ls = mf.analyze(ls)\n",
    "            ls = tg.analyze(ls)\n",
    "            if len(ls) > 0:\n",
    "                ws = ls[0].get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}_{w.get_tag()}'\n",
    "                    add_to_dict(key, results)\n",
    "    return results\n",
    "\n",
    "def process(text, token=True, lemma=False, pos=False):\n",
    "    features = {}\n",
    "    if token:\n",
    "        features['tokens'] = obtain_tokens(text)\n",
    "    if lemma:\n",
    "        features['lemmas'] = obtain_lemmas(text)\n",
    "    if pos:\n",
    "        features['PoS'] = obtain_pos(text)\n",
    "    return features\n",
    "\n",
    "def add_to_dict(key, feature_Dictionary):\n",
    "    if key in feature_Dictionary:\n",
    "        feature_Dictionary[key] += 1\n",
    "    else:\n",
    "        feature_Dictionary[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33832e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_to_dict(key, feature_Dictionary):\n",
    "    if key in feature_Dictionary:\n",
    "        feature_Dictionary[key] += 1\n",
    "    else:\n",
    "        feature_Dictionary[key] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dcb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreelingAnalyzer(object):\n",
    "    def __init__(self, folder, lang):\n",
    "        self.folder = folder\n",
    "        self.lang = lang\n",
    "        self.tk = None\n",
    "        self.sp = None\n",
    "        self.sid = None\n",
    "        self.mf = None\n",
    "        self.tg = None\n",
    "        self.sen = None\n",
    "        # self.parser = None\n",
    "        self.dep = None\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        # Check whether we know where to find FreeLing data files\n",
    "        if \"FREELINGDIR\" not in os.environ:\n",
    "            if sys.platform == \"win32\" or sys.platform == \"win64\":\n",
    "                os.environ[\"FREELINGDIR\"] = \"C:\\\\Program Files\"\n",
    "            else:\n",
    "                os.environ[\"FREELINGDIR\"] = \"/usr/local\"\n",
    "            print(\n",
    "                \"FREELINGDIR environment variable not defined, trying \",\n",
    "                os.environ[\"FREELINGDIR\"],\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "\n",
    "        if not os.path.exists(os.environ[\"FREELINGDIR\"] + \"/share/freeling\"):\n",
    "            print(\n",
    "                \"Folder\",\n",
    "                os.environ[\"FREELINGDIR\"] + \"/share/freeling\",\n",
    "                \"not found.\\n\" +\n",
    "                \"Please set FREELINGDIR environment variable to FreeLing installation directory\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Location of FreeLing configuration files.\n",
    "        DATA = os.environ[\"FREELINGDIR\"] + \"/share/freeling/\"\n",
    "\n",
    "        # Init locales\n",
    "        pyfreeling.util_init_locale(\"default\")\n",
    "\n",
    "        # create language detector. Used just to show it. Results are printed\n",
    "        # but ignored (after, it is assumed language is LANG)\n",
    "        # la = pyfreeling.lang_ident(DATA + \"common/lang_ident/ident-few.dat\")\n",
    "\n",
    "        # create options set for maco analyzer.\n",
    "        # Default values are Ok, except for data files.\n",
    "        LANG = self.lang\n",
    "        op = pyfreeling.maco_options(LANG)\n",
    "        op.set_data_files(\n",
    "            \"\",\n",
    "            DATA + \"common/punct.dat\",\n",
    "            DATA + LANG + \"/dicc.src\",\n",
    "            DATA + LANG + \"/afixos.dat\",\n",
    "            \"\",\n",
    "            DATA + LANG + \"/locucions.dat\",\n",
    "            DATA + LANG + \"/np.dat\",\n",
    "            DATA + LANG + \"/quantities.dat\",\n",
    "            DATA + LANG + \"/probabilitats.dat\",\n",
    "        )\n",
    "\n",
    "        # create analyzers\n",
    "        self.tk = pyfreeling.tokenizer(DATA + LANG + \"/tokenizer.dat\")\n",
    "        self.sp = pyfreeling.splitter(DATA + LANG + \"/splitter.dat\")\n",
    "        self.sid = self.sp.open_session()\n",
    "        self.mf = pyfreeling.maco(op)\n",
    "\n",
    "        # activate morpho modules to be used in next call\n",
    "        self.mf.set_active_options(\n",
    "            False, # UserMap\n",
    "            True,  # NumbersDetection\n",
    "            True,  # PunctuationDetection\n",
    "            True,  # DatesDetection\n",
    "            True,  # DictionarySearch\n",
    "            True,  # AffixAnalysis\n",
    "            False, # CompoundAnalysis\n",
    "            True,  # RetokContractions\n",
    "            True,  # MultiwordsDetection\n",
    "            True,  # NERecognition\n",
    "            True,  # QuantitiesDetection\n",
    "            True   # ProbabilityAssignment\n",
    "        )\n",
    "        # default: all created submodules are used\n",
    "\n",
    "        # create tagger, sense anotator, and parsers\n",
    "        self.tg = pyfreeling.hmm_tagger(DATA + LANG + \"/tagger.dat\", True, 2)\n",
    "        self.sen = pyfreeling.senses(DATA + LANG + \"/senses.dat\")\n",
    "        self.dep = pyfreeling.dep_lstm(\n",
    "            DATA + LANG + \"/dep_lstm/params-en.dat\")\n",
    "\n",
    "    def obtain_tokens(self, text, feature_dict):\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            if lin.strip():\n",
    "                lw = self.tk.tokenize(lin)\n",
    "                ls = self.sp.split(self.sid, lw, False)\n",
    "        for s in ls:\n",
    "            ws = s.get_words()\n",
    "            for w in ws:\n",
    "                key = w.get_form()\n",
    "                add_to_dict(key, feature_dict)\n",
    "        #return results\n",
    "        return ls\n",
    "    \n",
    "    def close(self):\n",
    "        self.sp.close_session(self.sid)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "683733e8",
   "metadata": {},
   "source": [
    "    def obtain_lemmas(self, text):\n",
    "        results = {}\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            if lin.strip():\n",
    "                lw = self.tk.tokenize(lin)\n",
    "                ls = self.sp.split(self.sid, lw, False)\n",
    "                ls = self.mf.analyze(ls)\n",
    "            if len(ls) > 0:\n",
    "                ws = ls[0].get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}_{w.get_lemma()}'\n",
    "                    add_to_dict(key, results)\n",
    "        return results\n",
    "\n",
    "    def obtain_pos(self, text):\n",
    "        results = {}\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            if lin.strip():\n",
    "                lw = self.tk.tokenize(lin)\n",
    "                ls = self.sp.split(self.sid, lw, False)\n",
    "                ls = self.tg.analyze(ls)\n",
    "            if len(ls) > 0:\n",
    "                ws = ls[0].get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}_{w.get_tag()}'\n",
    "                    add_to_dict(key, results)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_dict = {}\n",
    "email_list = []\n",
    "\n",
    "for directory, subdirectory, filenames in os.walk(basedir):\n",
    "    for filename in filenames:\n",
    "        raw_parse(os.path.join(directory, filename), email_list)\n",
    "email_list.sort(key=lambda x: x[0])\n",
    "\n",
    "# mail_dict = obtain_base_features(email_list)\n",
    "\n",
    "pureThreads = obtain_raw_threads(mail_dict, email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mailsWithFeatures = {}\n",
    "for mail in email_list[:2]:\n",
    "    actualEmail = mail[1]\n",
    "    mailsWithFeatures[mail] = obtain_base_features(actualEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8c2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ed692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreelingAnalyzer(object):\n",
    "    def __init__(self, folder, lang):\n",
    "        self.folder = folder\n",
    "        self.lang = lang\n",
    "        self.tk = None\n",
    "        self.sp = None\n",
    "        self.sid = None\n",
    "        self.mf = None\n",
    "        self.tg = None\n",
    "        self.sen = None\n",
    "        # self.parser = None\n",
    "        self.dep = None\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        # Check whether we know where to find FreeLing data files\n",
    "        if \"FREELINGDIR\" not in os.environ:\n",
    "            if sys.platform == \"win32\" or sys.platform == \"win64\":\n",
    "                os.environ[\"FREELINGDIR\"] = \"C:\\\\Program Files\"\n",
    "            else:\n",
    "                os.environ[\"FREELINGDIR\"] = \"/usr/local\"\n",
    "            print(\n",
    "                \"FREELINGDIR environment variable not defined, trying \",\n",
    "                os.environ[\"FREELINGDIR\"],\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "\n",
    "        if not os.path.exists(os.environ[\"FREELINGDIR\"] + \"/share/freeling\"):\n",
    "            print(\n",
    "                \"Folder\",\n",
    "                os.environ[\"FREELINGDIR\"] + \"/share/freeling\",\n",
    "                \"not found.\\n\" +\n",
    "                \"Please set FREELINGDIR environment variable to FreeLing installation directory\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Location of FreeLing configuration files.\n",
    "        DATA = os.environ[\"FREELINGDIR\"] + \"/share/freeling/\"\n",
    "\n",
    "        # Init locales\n",
    "        pyfreeling.util_init_locale(\"default\")\n",
    "\n",
    "        # create language detector. Used just to show it. Results are printed\n",
    "        # but ignored (after, it is assumed language is LANG)\n",
    "        # la = pyfreeling.lang_ident(DATA + \"common/lang_ident/ident-few.dat\")\n",
    "\n",
    "        # create options set for maco analyzer.\n",
    "        # Default values are Ok, except for data files.\n",
    "        LANG = self.lang\n",
    "        op = pyfreeling.maco_options(LANG)\n",
    "        op.set_data_files(\n",
    "            \"\",\n",
    "            DATA + \"common/punct.dat\",\n",
    "            DATA + LANG + \"/dicc.src\",\n",
    "            DATA + LANG + \"/afixos.dat\",\n",
    "            \"\",\n",
    "            DATA + LANG + \"/locucions.dat\",\n",
    "            DATA + LANG + \"/np.dat\",\n",
    "            DATA + LANG + \"/quantities.dat\",\n",
    "            DATA + LANG + \"/probabilitats.dat\",\n",
    "        )\n",
    "\n",
    "        # create analyzers\n",
    "        self.tk = pyfreeling.tokenizer(DATA + LANG + \"/tokenizer.dat\")\n",
    "        self.sp = pyfreeling.splitter(DATA + LANG + \"/splitter.dat\")\n",
    "        self.sid = self.sp.open_session()\n",
    "        self.mf = pyfreeling.maco(op)\n",
    "\n",
    "        # activate morpho modules to be used in next call\n",
    "        self.mf.set_active_options(\n",
    "            False, # UserMap\n",
    "            True,  # NumbersDetection\n",
    "            True,  # PunctuationDetection\n",
    "            True,  # DatesDetection\n",
    "            True,  # DictionarySearch\n",
    "            True,  # AffixAnalysis\n",
    "            False, # CompoundAnalysis\n",
    "            True,  # RetokContractions\n",
    "            True,  # MultiwordsDetection\n",
    "            True,  # NERecognition\n",
    "            True,  # QuantitiesDetection\n",
    "            True   # ProbabilityAssignment\n",
    "        )\n",
    "        # default: all created submodules are used\n",
    "\n",
    "        # create tagger, sense anotator, and parsers\n",
    "        self.tg = pyfreeling.hmm_tagger(DATA + LANG + \"/tagger.dat\", True, 2)\n",
    "        self.sen = pyfreeling.senses(DATA + LANG + \"/senses.dat\")\n",
    "        self.dep = pyfreeling.dep_lstm(\n",
    "            DATA + LANG + \"/dep_lstm/params-en.dat\")\n",
    "\n",
    "    def obtain_tokens_alt(self, text, feature_dict):\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            lw = self.tk.tokenize(lin.strip())\n",
    "            ls = self.sp.split(self.sid, lw, False)\n",
    "            #if lin.strip():\n",
    "             #   lw = self.tk.tokenize(lin)\n",
    "             #   ls = self.sp.split(self.sid, lw, False)\n",
    "            #if len(ls) > 0:\n",
    "            for s in ls:\n",
    "                ws = s.get_words()\n",
    "                for w in ws:\n",
    "                    key = w.get_form()\n",
    "                    add_to_dict(key, feature_dict)\n",
    "            #return results\n",
    "    \n",
    "    def obtain_tokens(self, text):\n",
    "        results = {}\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            lw = self.tk.tokenize(lin.strip())\n",
    "            ls = self.sp.split(self.sid, lw, False)\n",
    "            #if lin.strip():\n",
    "             #   lw = self.tk.tokenize(lin)\n",
    "             #   ls = self.sp.split(self.sid, lw, False)\n",
    "            #if len(ls) > 0:\n",
    "            for s in ls:\n",
    "                ws = s.get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}'\n",
    "                    add_to_dict(key, results)\n",
    "        return results\n",
    "    \n",
    "    def obtain_lemmas(self, text):\n",
    "        results = {}\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            lw = self.tk.tokenize(lin.strip())\n",
    "            ls = self.sp.split(self.sid, lw, False)\n",
    "            ls = self.mf.analyze(ls)\n",
    "            for s in ls:\n",
    "                ws = s.get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}_Lemma_{w.get_lemma()}'\n",
    "                    add_to_dict(key, results)\n",
    "        return results\n",
    "    \n",
    "    def obtain_pos(self, text):\n",
    "        results = {}\n",
    "        for lin in io.StringIO(text.get_payload()):\n",
    "            lw = self.tk.tokenize(lin.strip())\n",
    "            ls = self.sp.split(self.sid, lw, False)\n",
    "            ls = self.tg.analyze(ls)\n",
    "            for s in ls:\n",
    "                ws = s.get_words()\n",
    "                for w in ws:\n",
    "                    key = f'{w.get_form()}_PoS_{w.get_tag()}'\n",
    "                    add_to_dict(key, results)\n",
    "        return results\n",
    "    \n",
    "    def close(self):\n",
    "        self.sp.close_session(self.sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "anal = FreelingAnalyzer(basedir, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce892fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lin in io.StringIO(actualEmail.get_payload()):\n",
    "    print(lin.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb79644",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "#anal.obtain_tokens(actualEmail, results)\n",
    "#anal.obtain_tokens(actualEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de59549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = True\n",
    "mailsWithFeatures = {}\n",
    "for mail in email_list[:2]:\n",
    "    actualEmail = mail[1]\n",
    "    mailsWithFeatures[mail] = obtain_base_features(actualEmail)\n",
    "    if token:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['tokens'] = anal.obtain_tokens(actualEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mailsWithFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = True\n",
    "lemma = True\n",
    "mailsWithFeatures = {}\n",
    "for mail in email_list[:2]:\n",
    "    actualEmail = mail[1]\n",
    "    mailsWithFeatures[mail] = obtain_base_features(actualEmail)\n",
    "    if token:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['tokens'] = anal.obtain_tokens(actualEmail)\n",
    "    if lemma:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['lemmas'] = anal.obtain_lemmas(actualEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b867617",
   "metadata": {},
   "outputs": [],
   "source": [
    "mailsWithFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f818492",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = False\n",
    "lemma = False\n",
    "pos = True\n",
    "mailsWithFeatures = {}\n",
    "for mail in email_list[:2]:\n",
    "    actualEmail = mail[1]\n",
    "    mailsWithFeatures[mail] = obtain_base_features(actualEmail)\n",
    "    if token:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['tokens'] = anal.obtain_tokens(actualEmail)\n",
    "    if lemma:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['lemmas'] = anal.obtain_lemmas(actualEmail)\n",
    "    if pos:\n",
    "        mailsWithFeatures[mail][actualEmail['message-id']]['pos'] = anal.obtain_pos(actualEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79550e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "mailsWithFeatures"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7848ad99",
   "metadata": {},
   "source": [
    "sp.close_session(sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d166645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfm] *",
   "language": "python",
   "name": "conda-env-tfm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
