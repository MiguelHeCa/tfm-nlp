{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2461d6",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b898eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)\n",
    "\n",
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d69e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ = [list(tokenize(doc)) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf18361",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tg = [\n",
    "    TaggedDocument(words, ['d{}'.format(idx)])\n",
    "    for idx, words in enumerate(corpus_)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f75720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10468323 -0.11976366 -0.19807313  0.17087036  0.07115792]\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(corpus_tg, vector_size=5, min_count=0)\n",
    "print(model.dv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d49c79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Doc2Vec in module gensim.models.doc2vec:\n",
      "\n",
      "class Doc2Vec(gensim.models.word2vec.Word2Vec)\n",
      " |  Doc2Vec(documents=None, corpus_file=None, vector_size=100, dm_mean=None, dm=1, dbow_words=0, dm_concat=0, dm_tag_count=1, dv=None, dv_mapfile=None, comment=None, trim_rule=None, callbacks=(), window=5, epochs=10, shrink_windows=True, **kwargs)\n",
      " |  \n",
      " |  Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n",
      " |  \n",
      " |  Warnings\n",
      " |  --------\n",
      " |  This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n",
      " |  such as lambda functions etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Doc2Vec\n",
      " |      gensim.models.word2vec.Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, tag)\n",
      " |      Get the vector representation of (possible multi-term) tag.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tag : {str, int, list of str, list of int}\n",
      " |          The tag (or tags) to be looked up in the model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      np.ndarray\n",
      " |          The vector representations of each tag as a matrix (will be 1D if `tag` was a single tag)\n",
      " |  \n",
      " |  __init__(self, documents=None, corpus_file=None, vector_size=100, dm_mean=None, dm=1, dbow_words=0, dm_concat=0, dm_tag_count=1, dv=None, dv_mapfile=None, comment=None, trim_rule=None, callbacks=(), window=5, epochs=10, shrink_windows=True, **kwargs)\n",
      " |      Class for training, using and evaluating neural networks described in\n",
      " |      `Distributed Representations of Sentences and Documents <http://arxiv.org/abs/1405.4053v2>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      " |          Input corpus, can be simply a list of elements, but for larger corpora,consider an iterable that streams\n",
      " |          the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is\n",
      " |          left uninitialized -- use if you plan to initialize it in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |          Documents' tags are assigned automatically and are equal to line number, as in\n",
      " |          :class:`~gensim.models.doc2vec.TaggedLineDocument`.\n",
      " |      dm : {1,0}, optional\n",
      " |          Defines the training algorithm. If `dm=1`, 'distributed memory' (PV-DM) is used.\n",
      " |          Otherwise, `distributed bag of words` (PV-DBOW) is employed.\n",
      " |      vector_size : int, optional\n",
      " |          Dimensionality of the feature vectors.\n",
      " |      window : int, optional\n",
      " |          The maximum distance between the current and predicted word within a sentence.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling.\n",
      " |          In Python 3, reproducibility between interpreter launches also requires use of the `PYTHONHASHSEED`\n",
      " |          environment variable to control hash randomization.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      epochs : int, optional\n",
      " |          Number of iterations (epochs) over the corpus. Defaults to 10 for Doc2Vec.\n",
      " |      hs : {1,0}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupr√©, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      dm_mean : {1,0}, optional\n",
      " |          If 0 , use the sum of the context word vectors. If 1, use the mean.\n",
      " |          Only applies when `dm` is used in non-concatenative mode.\n",
      " |      dm_concat : {1,0}, optional\n",
      " |          If 1, use concatenation of context vectors rather than sum/average;\n",
      " |          Note concatenation results in a much-larger model, as the input\n",
      " |          is no longer the size of one (sampled or arithmetically combined) word vector, but the\n",
      " |          size of the tag(s) and all words in the context strung together.\n",
      " |      dm_tag_count : int, optional\n",
      " |          Expected constant number of document tags per document, when using\n",
      " |          dm_concat mode.\n",
      " |      dbow_words : {1,0}, optional\n",
      " |          If set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW\n",
      " |          doc-vector training; If 0, only trains doc-vectors (faster).\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |      shrink_windows : bool, optional\n",
      " |          New in 4.1. Experimental.\n",
      " |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
      " |          for each target word during training, to match the original word2vec algorithm's\n",
      " |          approximate weighting of context words by distance. Otherwise, the effective\n",
      " |          window size is always fixed to `window` words to either side.\n",
      " |      \n",
      " |      Some important internal attributes are the following:\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |          directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |      \n",
      " |      dv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          This object contains the paragraph vectors learned from the training data. There will be one such vector\n",
      " |          for each unique document tag supplied during training. They may be individually accessed using the tag\n",
      " |          as an indexed-access key. For example, if one of the training documents used a tag of 'doc003':\n",
      " |      \n",
      " |          .. sourcecode:: pycon\n",
      " |      \n",
      " |              >>> model.dv['doc003']\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Abbreviated name reflecting major configuration parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the models internal state.\n",
      " |  \n",
      " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of documents (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      " |          Can be simply a list of :class:`~gensim.models.doc2vec.TaggedDocument` elements, but for larger corpora,\n",
      " |          consider an iterable that streams the documents directly from disk/network.\n",
      " |          See :class:`~gensim.models.doc2vec.TaggedBrownCorpus` or :class:`~gensim.models.doc2vec.TaggedLineDocument`\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them). Documents' tags are assigned automatically\n",
      " |          and are equal to a line number, as in :class:`~gensim.models.doc2vec.TaggedLineDocument`.\n",
      " |      update : bool\n",
      " |          If true, the new words in `documents` will be added to model's vocab.\n",
      " |      progress_per : int\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additional key word arguments passed to the internal vocabulary construction.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Build model vocabulary from a passed dictionary that contains a (word -> word count) mapping.\n",
      " |      Words must be of type unicode strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          Word <-> count mapping.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during\n",
      " |          :meth:`~gensim.models.doc2vec.Doc2Vec.build_vocab` and is not stored as part of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of raw words in the vocabulary.\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the **specific** model's memory consuming members\n",
      " |          to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |          Includes members from the base classes as well as weights and tag lookup memory estimation specific to the\n",
      " |          class.\n",
      " |  \n",
      " |  estimated_lookup_memory(self)\n",
      " |      Get estimated memory for tag lookup, 0 if using pure int tags.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          The estimated RAM required to look up a tag in bytes.\n",
      " |  \n",
      " |  infer_vector(self, doc_words, alpha=None, min_alpha=None, epochs=None)\n",
      " |      Infer a vector for given post-bulk training document.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Subsequent calls to this function may infer different representations for the same document.\n",
      " |      For a more stable representation, increase the number of epochs to assert a stricter convergence.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc_words : list of str\n",
      " |          A document for which the vector representation will be inferred.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate. If unspecified, value from model initialization will be reused.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` over all inference epochs. If unspecified,\n",
      " |          value from model initialization will be reused.\n",
      " |      epochs : int, optional\n",
      " |          Number of times to train the new document. Larger values take more time, but may improve\n",
      " |          quality and run-to-run stability of inferred vectors. If unspecified, the `epochs` value\n",
      " |          from model initialization will be reused.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      np.ndarray\n",
      " |          The inferred paragraph vector for the new document.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors. Obsoleted.\n",
      " |      \n",
      " |      If you need a single unit-normalized vector for some key, call\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      " |      ``doc2vec_model.dv.get_vector(key, norm=True)``.\n",
      " |      \n",
      " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      " |          You lose information if you do this.\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Copy shareable data structures from another (possibly pre-trained) model.\n",
      " |      \n",
      " |      This specifically causes some structures to be shared, so is limited to\n",
      " |      structures (like those rleated to the known word/tag vocabularies) that\n",
      " |      won't change during training or thereafter. Beware vocabulary edits/updates\n",
      " |      to either model afterwards: the partial sharing and out-of-band modification\n",
      " |      may leave the other model in a broken state.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      " |          Other model whose internal data structures will be copied over to the current object.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, doctag_vec=False, word_vec=True, prefix='*dt_', fvocab=None, binary=False)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original C word2vec-tool.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in.\n",
      " |      doctag_vec : bool, optional\n",
      " |          Indicates whether to store document vectors.\n",
      " |      word_vec : bool, optional\n",
      " |          Indicates whether to store word vectors.\n",
      " |      prefix : str, optional\n",
      " |          Uniquely identifies doctags from word vocab, and avoids collision in case of repeated string in doctag\n",
      " |          and word vocab.\n",
      " |      fvocab : str, optional\n",
      " |          Optional file path used to save the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data will be saved in binary word2vec format, otherwise - will be saved in plain text.\n",
      " |  \n",
      " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, trim_rule=None)\n",
      " |      Create the models Vocabulary: A mapping from unique words in the corpus to their frequency count.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      " |          The tagged documents used to create the vocabulary. Their tags can be either str tokens or ints (faster).\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      progress_per : int\n",
      " |          Progress will be logged every `progress_per` documents.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during\n",
      " |          :meth:`~gensim.models.doc2vec.Doc2Vec.build_vocab` and is not stored as part of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      (int, int)\n",
      " |          Tuple of (Total words in the corpus, number of documents)\n",
      " |  \n",
      " |  similarity_unseen_docs(self, doc_words1, doc_words2, alpha=None, min_alpha=None, epochs=None)\n",
      " |      Compute cosine similarity between two post-bulk out of training documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      " |          An instance of a trained `Doc2Vec` model.\n",
      " |      doc_words1 : list of str\n",
      " |          Input document.\n",
      " |      doc_words2 : list of str\n",
      " |          Input document.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      epochs : int, optional\n",
      " |          Number of epoch to train the new document.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          The cosine similarity between `doc_words1` and `doc_words2`.\n",
      " |  \n",
      " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights.\n",
      " |      \n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of documents) or `total_words` (count of\n",
      " |      raw words in documents) **MUST** be provided. If `documents` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once,\n",
      " |      you can set `epochs=self.iter`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      " |          Can be simply a list of elements, but for larger corpora,consider an iterable that streams\n",
      " |          the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is\n",
      " |          left uninitialized -- use if you plan to initialize it in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them). Documents' tags are assigned automatically\n",
      " |          and are equal to line number, as in :class:`~gensim.models.doc2vec.TaggedLineDocument`.\n",
      " |      total_examples : int, optional\n",
      " |          Count of documents.\n",
      " |      total_words : int, optional\n",
      " |          Count of raw words in documents.\n",
      " |      epochs : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to `train`.\n",
      " |          Use only if making multiple calls to `train`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to\n",
      " |          :meth:`~gensim.models.doc2vec.Doc2Vec.train`.\n",
      " |          Use only if making multiple calls to :meth:`~gensim.models.doc2vec.Doc2Vec.train`, when you want to manage\n",
      " |          the alpha learning-rate yourself (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in documents.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.doc2vec.Doc2Vec` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      *args : object\n",
      " |          Additional arguments, see `~gensim.models.word2vec.Word2Vec.load`.\n",
      " |      **kwargs : object\n",
      " |          Additional arguments, see `~gensim.models.word2vec.Word2Vec.load`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.doc2vec.Doc2Vec.save`\n",
      " |          Save :class:`~gensim.models.doc2vec.Doc2Vec` model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  dbow\n",
      " |      Indicates whether 'distributed bag of words' (PV-DBOW) will be used, else 'distributed memory'\n",
      " |      (PV-DM) is used.\n",
      " |  \n",
      " |  dm\n",
      " |      Indicates whether 'distributed memory' (PV-DM) will be used, else 'distributed bag of words'\n",
      " |      (PV-DBOW) is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  docvecs\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.word2vec.Word2Vec:\n",
      " |  \n",
      " |  add_null_word(self)\n",
      " |  \n",
      " |  create_binary_tree(self)\n",
      " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      " |      word counts. Frequent words will have shorter binary codes.\n",
      " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  make_cum_table(self, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |      \n",
      " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Note this performs a CBOW-style propagation, even in SG models,\n",
      " |      and doesn't quite weight the surrounding words the same as in\n",
      " |      training -- so it's just one crude way of using a trained model\n",
      " |      as a predictor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of (str and/or int)\n",
      " |          List of context words, which may be words themselves (str)\n",
      " |          or their index in `self.wv.vectors` (int).\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |      \n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |      \n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |  \n",
      " |  prepare_weights(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  seeded_vector(self, seed_string, vector_size)\n",
      " |  \n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [list(tokenize(doc)) for doc in corpus]\n",
    "corpus = [\n",
    "    TaggedDocument(words, ['d{}'.format(idx)])\n",
    "    for idx, words in enumerate(corpus)\n",
    "]\n",
    "\n",
    "model = Doc2Vec(corpus, size=5, min_count=0)\n",
    "print(model.docvecs[0])\n",
    "# [ 0.01797447 -0.01509272  0.0731937   0.06814702 -0.0846546 ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miri] *",
   "language": "python",
   "name": "conda-env-miri-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
