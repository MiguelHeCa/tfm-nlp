{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a5bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5256990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('enron_mails.p')\n",
    "train, test = train_test_split(df, test_size=0.4, random_state=2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da20ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(input_docs, tokens_only=False):\n",
    "    for i, text in input_docs:\n",
    "        tokens = gensim.utils.simple_preprocess(text)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # Tags for training data\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "            \n",
    "train_corpus = list(create_corpus(train['text'].items()))\n",
    "test_corpus = list(create_corpus(test['text'].items(), tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7434183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eccfc9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef401a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 37s, sys: 3min 8s, total: 22min 45s\n",
      "Wall time: 10min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97186cd3",
   "metadata": {},
   "source": [
    "%%time\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "    \n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405fe317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (310439): «fyi forwarded by steven kean na enron on am kim frumkin ees am to steven kean na enron enron cc subject thank you steve thank you for participating once again in the video for deal central the videos are created housed and maintained in beth tilney area mentioned to beth that corporate may have an interest in sharing utilizing the ees video equipment and expertise robert pearson our in house film expert has masters in video programming taping etc and the good news within the next few weeks the editing equipment will be fully up and running once installed and operational all in house produced videos will have the look and feel of those created by outside vendors please feel free to contact beth ext if you have an interest in exploring this concept and thank you again for your participation best kim»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDocument (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m): «\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m»\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(doc_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(train_corpus[doc_id]\u001b[38;5;241m.\u001b[39mwords)))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSIMILAR/DISSIMILAR DOCS PER MODEL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m model)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, index \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMOST\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSECOND-MOST\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEDIAN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43msims\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEAST\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sims) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)]:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: «\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m»\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (label, sims[index], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(train_corpus[sims[index][\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mwords)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sims' is not defined"
     ]
    }
   ],
   "source": [
    "doc_id = len(train_corpus) - 1\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a70c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (56666): «business highlights enron global markets coal and emissions trading the first enron online synfuel trade occurred this past week establishing enrononline as the only online marketplace for synfuel the purpose of bringing synfuel on enrononline is to provide market transparency between the coal synfuel spread the coal cash book which began trading only two months ago has traded over million spot tons the fundamental purpose of this book is to capture short term market discrepancies while also adding liquidity to the spot market to further long term trading and marketing of coal enron energy and operational services ena solid fuel initiative ena started development of coal initiative late last year to support the electricity trading desk and to provide enron with hedge in the event that the cost of natural gas continues to escalate above current long term projections because of the culture shock associated the use of the words enron and coal in the same sentence consideration was been given to changing the name to dng dense natural gas however we went with solid fuel initiative this initiative is developing multiple sites throughout the country where ena feels there is the greatest opportunity for long term corporate benefit the primary areas are texas florida southeastern ohio and the central rocky mountain area there are principal areas currently being aggressively pursued brownsville tx and brighton fl in brownsville the team secured formal owner approval to lease acre site near the brownsville ship channel this approval will allow the formal permitting activities to proceed without delay mw facility is being considered for brownsville as conceptualized it will consist of two mw net cfb units circulating fluidized bed boilers the boilers will be fired by petroleum coke petcoke and use technology that will generate electricity in the most environmentally friendly way currently possible one of the advantages of locating this facility near brownsville is that it remains steady growth area and there is already serious interest by others to take substantial initial base load capacity there is further synergy with other developing projects in the area that could lock down additional base capacity as stated the plant is initially mw but will be designed to be expanded to mw facility the eeos team is starting to grow and currently includes hilda akin cheryl kent project admin assistants bill fox as pem and paula solomon as pcm as the project develops there will be more team members required in addition as the florida opportunity develops we plan to establish separate team esource esource enron research group recently posted on its website forrester report as well as analyst reports for various industry and companies these reports are free and available to all enron employees on the esource hot topics page at http esource enron com hot_topics asp http esource enron com hot_topics asp to date over employees have downloaded spending wisely on private hubs report valued at over employees to date have viewed the analyst reports provided by investext the reports which are updated continuously are presented in folders by company and industry new folders are being added upon request to submit suggestion for new folder please complete the suggestion form at http esource enron com suggestion asp http esource enron com suggestion asp watch for the new esource bulletin the source it will feature frequently requested searches tips on finding information new products services and events meanwhile to further fulfill enron information needs esource has arranged for analysts and bureau chiefs of various organizations and publishing houses to offer seminars to enron employees enron freight markets efm signed firm contract with streamline shippers association last week to provide truckload capacity for one year with efm option to extend the contract for up to an additional year streamline transloads international shipments that enter the through the ports of long beach and los angeles streamline then consolidates the shipments into full truckloads destined to the major population centers throughout the country efm will begin moving shipments to northeast destinations this week in the news we are pleased to announce that sally beck has been named chief operating officer for enron net works she will join greg piper president and chief executive officer and mark pickering chief technology officer in the enron net works office of the chair in her current role as managing director for enron net works sally heads up enron global risk management operations she brings more than years professional experience to enron joining the company in she graduated from the university of texas at austin with in marketing and an mba with concentration in finance welcome new hires egm robert bogucki adam gianonne eim sylvia miller ena theresa mcbride sriram vasudevan jonathan mckay transfers to or within ena brandon cavazos egm michael garcia nuggets notes congratulations to rahul kumar for passing the cfa level iii exam weather risk management the weather group is pleased to welcome several new people to their group trevor nathan and norm trethewey have joined the australia office and kaoru hijikata has joined the tokyo office please welcome them to our team enrononline figures below are the latest figures for enrononline as of september total life to date transactions life to date notional value of transactions billion enron best practice tips united parcel service ups delivers the following air products ups next day air early to selected zip codes ups next day air guaranteed by ups next day air saver guaranteed by ups nd day air am guaranteed by ups nd day air guaranteed by end of second day ups day select guaranteed by end of third day ups international service world wide express plus by to select cities in europe ups world wide express second business day for questions about selected cities and zip codes please call the enron preferred customer associate at ups is enron preferred vendor for express deliveries legal stuff the information contained in this newsletter is confidential and proprietary to enron corp and its subsidiaries it is intended for internal use only and should not be disclosed embedded paintbrush picture»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "# sim_id = second_ranks[doc_id]\n",
    "#print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c917db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545bacb",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c984d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfm] *",
   "language": "python",
   "name": "conda-env-tfm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
